{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DicomBeamComposer_Integration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buaindra/cloud-learning-with-python/blob/dev01/HealthcareAPI_Dicom_Beam_Composer_Python/DicomBeamComposer_Integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration file (**dicom_config.properties**)\n",
        "> location: /home/airflow/gcs/data/dicom_config.properties"
      ],
      "metadata": {
        "id": "1z33zpUSTh_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "``` python3\n",
        "[QA]\n",
        "project_id =\n",
        "sa_key_file = gs://<bucket name>/<prefix>/<key-json file> \n",
        "gcs_python_bq_to_dw = gs://<bucket name>/<prefix>/<py file> \n",
        "setup_file = /home/airflow/gcs/dags/<foldername>/setup.py\n",
        "extra_package = /home/airflow/gcs/dags/<foldername>/<packagename-version_name-.tar.gz>\n",
        "```"
      ],
      "metadata": {
        "id": "b5V6B7_9T9fD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Dataflow"
      ],
      "metadata": {
        "id": "0pIh8xsedBYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Pipeline Code in Python"
      ],
      "metadata": {
        "id": "Rfa-xOVlc19R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to pass sa key json (gcs path) as side input to pardo function\n",
        "#### Healthcare API import/export call with side_input, pardo"
      ],
      "metadata": {
        "id": "RP_NyUzwNfKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gcs to hc\n",
        "import json\n",
        "import argparse\n",
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions, SetupOptions, GoogleCloudOptions\n",
        "from apache_beam.options.pipeline_options import StandardOptions, DebugOptions, DirectOptions\n",
        "from apache_beam.options.pipeline_options import WorkerOptions\n",
        "import configparser\n",
        "\n",
        "# take variables values from configuration file\n",
        "env = \"QA\"\n",
        "config = configparser.ConfigParser()\n",
        "config.read(\"/home/airflow/gcs/data/dicom_config.properties\")\n",
        "\n",
        "input_dict = {}\n",
        "input_dict[\"project_id\"] = config[env][\"project_id\"]\n",
        "input_dict[\"region\"] = config[env][\"region\"]\n",
        "input_dict[\"hc_dataset_id\"] = config[env][\"hc_dataset_id\"]\n",
        "input_dict[\"hc_dicom_store_id\"] = config[env][\"hc_dicom_store_id\"]\n",
        "input_dict[\"dicom_gcs_bucket\"] = config[env][\"dicom_gcs_bucket\"]\n",
        "input_dict[\"dicom_gcs_prefix\"] = config[env][\"dicom_gcs_prefix\"]\n",
        "\n",
        "def get_list_bucket_prefix(bucket_nm, prefix_nm):\n",
        "  from google.cloud import storage\n",
        "\n",
        "  delimiter = \"/\"\n",
        "  dicom_wildcard = \"**.dcm\"\n",
        "  storage_client = storage.Client(project=input_dict[\"project_id\"])\n",
        "  bucket = storage_client.get_bucket(bucket_nm)\n",
        "  iterator = bucket.list_blobs(delimiter=delimiter, prefix=prefix_nm)\n",
        "  list(iterator) # Need for iteration\n",
        "  for prefix in iterator.prefixes:\n",
        "    out = f\"gs://{bucket_nm}/{prefix}{dicom_wildcard}\"\n",
        "    yield out\n",
        "\n",
        "class ImportToDicomStore(beam.DoFn):\n",
        "  #def __init__(self, log):\n",
        "  #  self.log = log :\n",
        "  #  super(self.__class__, self).__init__() :\n",
        "  def process(self, dc_element, hc_element, creds):\n",
        "    from googleapiclient import discovery\n",
        "    from google.oauth2.service_account import Credentials\n",
        "    import json\n",
        "    import time\n",
        "\n",
        "    time.sleep(5) # to reduce the api call per mint/per user/per region\n",
        "\n",
        "    credentials_json = json.loads(\"\\n\".join(creds))\n",
        "    #credentials = Credentials.from_service_account_file(cw_sa_key_file) # local relative path of the json key file\n",
        "    credentials = Credentials.from_service_account_info(credentials_json)\n",
        "    hc_client = discovery.build(serviceName=\"healthcare\", version=\"v1\", credentials=credentials)\n",
        "    dicom_store_name = \"projects/{}/locations/{}/datasets/{}/dicomStores/{}\".format(\n",
        "        hc_element[\"project_id\"], hc_element[\"region\"], hc_element[\"hc_dataset_id\"], hc_element[\"hc_dicom_store_id\"]\n",
        "    )\n",
        "    body = {\"gcsSource\": {\"uri\": \"{}\".format(dc_element)}}\n",
        "    request_dicom_import = (\n",
        "        hc_client.projects()\n",
        "        .locations()\n",
        "        .datasets()\n",
        "        .dicomStores()\n",
        "        .import_(name=dicom_store_name, body=body)\n",
        "    )\n",
        "    response_dicom_import = request_dicom_import.execute()\n",
        "    yield response_dicom_import\n",
        "\n",
        "\n",
        "def run(argv=None):\n",
        "  # setup beam argument\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\"--sa_key\",\n",
        "                      dest=\"sa_key\",\n",
        "                      required=True,\n",
        "                      help=\"service account credentials json\"    \n",
        "  )\n",
        "  args, beam_args = parser.parse_known_args(argv)\n",
        "  pipeline_options = PipelineOptions(beam_args)\n",
        "\n",
        "  google_cloud_options = pipeline_options.view_as(GoogleCloudOptions)\n",
        "  google_cloud_options.project = config[env][\"project\"]\n",
        "  google_cloud_options.region = config[env][\"region\"]\n",
        "  google_cloud_options.job_name = config[env][\"job_name\"]\n",
        "  google_cloud_options.staging_location = config[env][\"staging_location\"]\n",
        "  google_cloud_options.temp_location = config[env][\"temp_location\"]\n",
        "  google_cloud_options.service_account_email = config[env][\"service_account_email\"]\n",
        "\n",
        "  pipeline_options.view_as(DebugOptions).experiments = [\"use_unsupported_python_version\"]\n",
        "  pipeline_options.view_as(StandardOptions).runner = \"DataflowRunner\" # \"DirectRunner\" \n",
        "  #pipeline_options.view_as(DirectOptions).direct_num_workers = 3 \n",
        "  pipeline_options.view_as(WorkerOptions).num_workers = int(config[env][\"num_workers\"])\n",
        "  pipeline_options.view_as(WorkerOptions).max_num_workers = int(config[env][\"max_num_workers\"])\n",
        "  #pipeline_options.view_as(WorkerOptions).autoscaling_algorithm = \"THROUGHPUT_BASED\"\n",
        "  pipeline_options.view_as(SetupOptions).save_main_session = True\n",
        "  #pipeline_options.view_as(SetupOptions).setup_file = \"./setup.py\"\n",
        "  #pipeline_options.view_as(SetupOptions).extra_packages = [\"dist/<packageName-0.1.0.tar.gz\"]\n",
        "\n",
        "  #debug\n",
        "  import os\n",
        "  p = os.path.realpath(__file__)\n",
        "  print(\"beam pipeline path:\", p)\n",
        "\n",
        "  # pipeline obj initiated\n",
        "  with beam.Pipeline(options=pipeline_options) as pipeline:\n",
        "    pc_hc_conf =( pipeline | \"Load HC Config\" >> beam.Create([input_dict]))\n",
        "    pc_credentials = ( pipeline | \"Read Credentials from GCS\" >> beam.io.textio.ReadFromText(args.sa_key))\n",
        "    pc_main = (\n",
        "        pipeline | \"Added Dicom Input Lists\" >> beam.Create(get_list_bucket_prefix(input_dict[\"dicom_gcs_bucket\"], input_dict[\"dicom_gcs_prefix\"]))\n",
        "                 | \"HC Import API\" >> beam.ParDo(ImportToDicomStore(), beam.pvalue.AsSingletone(pc_hc_conf), beam.pvalue.AsList(pc_credentials))\n",
        "                 | \"Print Output\" >> beam.Map(print)\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  run()"
      ],
      "metadata": {
        "id": "lYM8X8dP20fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hc to bq\n",
        "from apache_beam.io.gcp.bigquery import BigQueryDisposition\n",
        "\n",
        "class Export_HC_Instance(beam.DoFn):\n",
        "  def process(self, element, creds):\n",
        "    from googleapiclient import discovery\n",
        "    from google.oauth2.service_account import Credentials\n",
        "    import json\n",
        "\n",
        "    credentials_json = json.loads(\"\\n\".join(creds))\n",
        "    credentials = Credentials.from_service_account_info(credentials_json)\n",
        "\n",
        "    hc_client = discovery.build(serviceName=\"healthcare\", version=\"v1\", credentials=credentials)\n",
        "\n",
        "    dicom_store_name = \"projects/{}/locations/{}/datasets/{}/dicomStores/{}\".format(\n",
        "        element[\"project_id\"], element[\"region\"], element[\"hc_dataset_id\"], element[\"hc_dicom_store_id\"]\n",
        "    )\n",
        "\n",
        "    body = {\"bigqueryDestination\": {\"tableUri\": \"bq://{}.{}.{}\".format(element[\"project_id\"], element[\"bq_dataset\"], element[\"bq_table\"]), \"writeDisposition\": BigQueryDisposition.WRITE_APPEND}}\n",
        "    #body = {\"gcsDestination\": {\"uriPrefix\": \"gs://<bucket_name>/<sub_folder>\"}}\n",
        "    \n",
        "    request = (\n",
        "        hc_client.projects()\n",
        "        .locations()\n",
        "        .datasets()\n",
        "        .dicomStores()\n",
        "        .export(name=dicom_store_name, body=body)\n",
        "    )\n",
        "    response = request.execute()\n",
        "    yield response"
      ],
      "metadata": {
        "id": "cwOMlhkySpV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### setup.py and extra_packages call from beam pipeline"
      ],
      "metadata": {
        "id": "3CUl535zZbG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bq to dw\n",
        "import json\n",
        "import argparse\n",
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions, SetupOptions, GoogleCloudOptions\n",
        "from apache_beam.options.pipeline_options import StandardOptions, DebugOptions, DirectOptions\n",
        "from apache_beam.options.pipeline_options import WorkerOptions\n",
        "import configparser\n",
        "\n",
        "# take variables values from configuration file\n",
        "env = \"QA\"\n",
        "config = configparser.ConfigParser()\n",
        "config.read(\"/home/airflow/gcs/data/dicom_config.properties\")\n",
        "\n",
        "project_number = config[env][\"project_number\"]\n",
        "location = config[env][\"location\"]\n",
        "bq_table = config[env][\"bq_table\"]\n",
        "bq_dataset = config[env][\"bq_dataset\"]\n",
        "bq_project = config[env][\"bq_project\"]\n",
        "\n",
        "class ConvertToJsonText(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    dict_element = json.dumps(dict(element), indent=2, default=str)\n",
        "    doc_name = element[\"SOPInstanceUID\"].replace(\".\",\"_\")\n",
        "    yield {\"dicom_element\": dict_element, \"doc_name\": str(doc_name)}\n",
        "\n",
        "class ImportToContentWarehouse(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    from healthcare_lib import doc_lib\n",
        "    doc_name = \"dicom_\" + element[\"doc_name\"] + \".txt\"\n",
        "    dicom_json_text = str(element[\"dicom_element\"])\n",
        "    schema_json_text = \"\"\n",
        "\n",
        "    returned_schemas = doc_lib.return_schemes_names(\n",
        "        project_number=project_number, location=location, display_name=\"dicom\"\n",
        "    )\n",
        "\n",
        "    if not returned_schemas:\n",
        "      document_schema = doc_lib.upload_document_schema(\n",
        "          project_number=project_number, location=location, schema_name=bq_table, schema_json_text=schema_json_text, endpoint_override=\"\" \n",
        "      )\n",
        "    else:\n",
        "      document_schema = returned_schemas[0]\n",
        "\n",
        "    doc_lib.upload_document(\n",
        "        project_number=project_number, location=location, endpoint_override=\"\", document_schema=document_schema,\n",
        "        doc_name = doc_name, doc_json_text=dicom_json_text\n",
        "    )\n",
        "    yield doc_name\n",
        "\n",
        "\n",
        "def run(argv=None):\n",
        "  # setup beam argument\n",
        "  parser = argparse.ArgumentParser()\n",
        "  args, beam_args = parser.parse_known_args(argv)\n",
        "  pipeline_options = PipelineOptions(beam_args)\n",
        "\n",
        "  google_cloud_options = pipeline_options.view_as(GoogleCloudOptions)\n",
        "  google_cloud_options.project = config[env][\"project\"]\n",
        "  google_cloud_options.region = config[env][\"region\"]\n",
        "  google_cloud_options.job_name = config[env][\"job_name\"]\n",
        "  google_cloud_options.staging_location = config[env][\"staging_location\"]\n",
        "  google_cloud_options.temp_location = config[env][\"temp_location\"]\n",
        "  google_cloud_options.service_account_email = config[env][\"service_account_email\"]\n",
        "\n",
        "  pipeline_options.view_as(DebugOptions).experiments = [\"use_unsupported_python_version\"]\n",
        "  pipeline_options.view_as(StandardOptions).runner = \"DataflowRunner\" # \"DirectRunner\" \n",
        "  #pipeline_options.view_as(DirectOptions).direct_num_workers = 3 \n",
        "  pipeline_options.view_as(WorkerOptions).num_workers = int(config[env][\"num_workers\"])\n",
        "  pipeline_options.view_as(WorkerOptions).max_num_workers = int(config[env][\"max_num_workers\"])\n",
        "  #pipeline_options.view_as(WorkerOptions).autoscaling_algorithm = \"THROUGHPUT_BASED\"\n",
        "  pipeline_options.view_as(SetupOptions).save_main_session = True\n",
        "  #pipeline_options.view_as(SetupOptions).setup_file = \"./setup.py\"\n",
        "  #pipeline_options.view_as(SetupOptions).extra_packages = [\"dist/<packageName-0.1.0.tar.gz\"]\n",
        "\n",
        "  #debug\n",
        "  import os\n",
        "  p = os.path.realpath(__file__)\n",
        "  print(\"beam pipeline path:\", p)\n",
        "\n",
        "  # pipeline obj initiated\n",
        "  with beam.Pipeline(options=pipeline_options) as pipeline:\n",
        "\n",
        "    pc = (\n",
        "        pipeline | \"Read from BQ\" >> beam.io.ReadFromBigQuery(\n",
        "                      table = bq_table,\n",
        "                      dataset = bq_dataset,\n",
        "                      project = bq_project)\n",
        "                 | \"Convert to JsonText\" >> beam.ParDo(ConvertToJsonText())\n",
        "                 | \"Load to DW\" >> beam.ParDo(ImportToContentWarehouse())\n",
        "                 | \"Print Output\" >> beam.Map(print)\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  run()"
      ],
      "metadata": {
        "id": "hyilBmv4QS0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Composer\n"
      ],
      "metadata": {
        "id": "R0IhxFXhQTcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### setup.py\n",
        "\n",
        "``` python3\n",
        "from setuptools import setup, find_packages\n",
        "\n",
        "setup(name = \"healthcare_package\",\n",
        "  version=\"0.2.0\",\n",
        "  packages=find_packages(),\n",
        "  install_requires=[\"google-cloud-storage==2.1.0\", \"google-cloud-documentai==1.2.1\"]\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "N-fY4HwY28mG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### how to call dependent python lib into beam pipeline, with setup.py"
      ],
      "metadata": {
        "id": "sSxLMmAtNtwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Composer Dag Code in Python-"
      ],
      "metadata": {
        "id": "94hE3jEpQ1Ej"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zq45OyXDPn9-"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import configparser\n",
        "from airflow import DAG\n",
        "from airflow import models\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from airflow.operators.dummy_operator import DummyOperator\n",
        "from airflow.providers.apache.beam.operators.beam import BeamRunPythonPipelineOperator\n",
        "import logging\n",
        "\n",
        "log_name = \"composer_job\"\n",
        "logger = logging.getLogger(log_name)\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "# take variables values from configuration file\n",
        "#env = \"QA\"\n",
        "env=os.environ[\"ENV\"]\n",
        "config = configparser.ConfigParser()\n",
        "#config.read(\"/home/airflow/gcs/data/dicom_config.properties\")\n",
        "config.read(f\"/home/airflow/gcs/data/dicom_config_{env.lower()}.properties\")\n",
        "\n",
        "project_id = config[env][\"project_id\"]\n",
        "region = config[env][\"region\"]\n",
        "hc_dataset_id = config[env][\"hc_dataset_id\"]\n",
        "bq_table = config[env][\"hc_dataset_id\"]\n",
        "bq_dataset = config[env][\"hc_dataset_id\"]\n",
        "sa_key_file = config[env][\"sa_key_file\"]\n",
        "gcs_python_bq_to_dw = config[env][\"gcs_python_bq_to_dw\"] # location of the dataflow python file gs://<bucket name>/<prefix>/<py file> \n",
        "\n",
        "cur_date = datetime.today() # datetime.now()\n",
        "dicom_store_id = config[env][\"dicom_store_id\"] + \"_\" + cur_date.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
        "\n",
        "setup_file = config[env][\"setup_file\"]\n",
        "extra_package = config[env][\"extra_package\"]\n",
        "\n",
        "def create_dicom_store(project_id, location, dataset_id, dicom_store_id):\n",
        "    from googleapiclient import discovery # not require explicit package installation in composer 2 airflow 2\n",
        "    api_version = \"v1\"\n",
        "    service_name = \"healthcare\"\n",
        "    client = discovery.build(service_name, api_version)\n",
        "    dicom_store_parent = \"projects/{}/locations/{}/datasets/{}\".format(project_id, location, dataset_id)\n",
        "    request = (\n",
        "        client.projects()\n",
        "        .locations()\n",
        "        .datasets()\n",
        "        .dicomStores()\n",
        "        .create(parent=dicom_store_parent, body={}, dicomStoreID=dicom_store_id)\n",
        "    )\n",
        "    response = request.execute()\n",
        "    #context[\"ti\"].xcom_push(\"dicom_store_id\", response.get(\"name\", \"\").split(\"/\")[-1]) #xcom_push\n",
        "    return response[\"name\"].split(\"/\")[-1] # will return the created dicom store name\n",
        "\n",
        "def create_bq_column(bq_project, bq_dataset, bq_table):\n",
        "    from google.cloud import bigquery\n",
        "    bq_client = bigquery.Client()\n",
        "    table = bq_client.get_table(f\"{bq_project}.{bq_dataset}.{bq_table}\")\n",
        "    original_schema = table.schema\n",
        "    flag = False\n",
        "    for i in original_schema:\n",
        "        if i.name == \"ingestion_datetime\":\n",
        "            flag = False\n",
        "            break\n",
        "        else:\n",
        "            flag = True\n",
        "    if flag:\n",
        "        new_schema = original_schema[:]\n",
        "        new_schema.append(bigquery.SchemaField(\"ingestion_datetime\", \"datetime\", mode=\"NULLABLE\")) # add new column to schema\n",
        "        table.schema = new_schema\n",
        "        table = bq_client.update_table(table, [\"schema\"])\n",
        "\n",
        "def update_bq_column(bq_project, bq_dataset, bq_table):\n",
        "    from google.cloud import bigquery\n",
        "    bq_client = bigquery.Client()\n",
        "    bq_ingestion_dt = datetime.today()\n",
        "    dml_query = (f\"UPDATE `{bq_project}.{bq_dataset}.{bq_table}` SET ingestion_datetime = '{bq_ingestion_dt}' WHERE ingestion_datetime is NULL\")\n",
        "    query_job = bq_client.query(dml_query)\n",
        "    query_job.result() # waits for the statement to finish\n",
        "\n",
        "# specify the default argument\n",
        "default_args ={\n",
        "    \"owner\": \"Airflow\",\n",
        "    \"depends_on_past\": False,\n",
        "    \"start_date\": datetime.today(),\n",
        "    \"retries\": 0,\n",
        "    \"retry_delay\": timedelta(minutes=5)\n",
        "}\n",
        "\n",
        "with models.DAG(\n",
        "    dag_id = \"composer_job_name\",\n",
        "    default_args = default_args,\n",
        "    schedule_interval = None, #timedelta(days=1) \n",
        "    #schedule_interval = \"@daily\"\n",
        ") as dag:\n",
        "\n",
        "    start = DummyOperator(\n",
        "        task_id = \"Start\"\n",
        "    )\n",
        "\n",
        "    create_dicom_store = PythonOperator(\n",
        "        task_id = \"create_dicom_store\",\n",
        "        provide_context=True,\n",
        "        python_callable = create_dicom_store, \n",
        "        op_kwargs={\"project_id\": project_id, \"location\": region, \"dataset_id\": hc_dataset_id, \"dicom_store_id\": dicom_store_id},\n",
        "        dag=dag\n",
        "    )\n",
        "\n",
        "    wait_dag = BashOperator(\n",
        "        task_id = \"wait_dag\",\n",
        "        bash_command = \"sleep 10s\"\n",
        "    )\n",
        "\n",
        "    create_bq_column = PythonOperator(\n",
        "        task_id = \"create_bq_column\",\n",
        "        provide_context=True,\n",
        "        python_callable = create_bq_column, \n",
        "        op_kwargs={\"bq_project\": project_id, \"bq_dataset\": bq_dataset, \"bq_table\": bq_table},\n",
        "        dag=dag\n",
        "    )\n",
        "\n",
        "    update_bq_column = PythonOperator(<same as above>)\n",
        "\n",
        "    beam_pipeline = BeamRunPythonPipelineOperator(\n",
        "        task_id = \"beam_pipeline\",\n",
        "        runner = \"DataflowRunner\",\n",
        "        py_file = gcs_python_bq_to_dw,\n",
        "        py_options = [],\n",
        "        pipeline_options = {\n",
        "            \"setup_file\": setup_file,\n",
        "            \"extra_package\": extra_package,\n",
        "            \"sa_key\": sa_key_file,\n",
        "            \"dicom_store_id\": \"{{ ti.xcom_pull(task_ids='create_dicom_store', key='return_value') }}\",\n",
        "            # jinja templat for xcom_pull, return_value is default key if you return anything from the method\n",
        "        },\n",
        "        py_requirements = [\"apache-beam[gcp]==2.34.0\", \"google-cloud-storage==2.1.0\", \"google-api-python-client==2.39.0\", \"google-cloud-logging==3.0.0\"],\n",
        "        py_interpreter = \"python3\",\n",
        "        dataflow_config = {\n",
        "            \"job_name\": \"dataflow_job_name\",\n",
        "            \"wait_until_finished\": True,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    end = DummyOperator(\n",
        "        task_id = \"End\"\n",
        "    )\n",
        "\n",
        "    start >> beam_pipeline >> end"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cloud Logging inside beam(dataflow) or airflow(composer)\n"
      ],
      "metadata": {
        "id": "FxSaddyhbDYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import google.cloud.logging\n",
        "from google.cloud.logging_v2.handlers import CloudLoggingHandler\n",
        "\n",
        "def get_logger(log_name: str):\n",
        "  gcloud_logging_client = google.cloud.logging.Client()\n",
        "  gcloud_logging_handler = CloudLoggingHandler(\n",
        "      gcloud_logging_client, name=log_name\n",
        "  )\n",
        "  #create a stream handler to log message to the console\n",
        "  stream_handler = logging.StreamHandler()\n",
        "  stream_handler.setLevel(logging.WARNING)\n",
        "  # now create a logger and add the handlers\n",
        "  logger = logging.getLogger(log_name)\n",
        "  logger.setLevel(logging.DEBUG)\n",
        "  logger.addHandler(gcloud_logging_handler)\n",
        "  logger.addHandler(stream_handler)\n",
        "  return  logger\n",
        "\n",
        "log = get_logger(\"beam-pipeline\")\n",
        "log.info(\"started pipeline\")\n"
      ],
      "metadata": {
        "id": "n7eOFqehbD31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Api call response handling"
      ],
      "metadata": {
        "id": "AJJb8MWU08sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class checkCompletion(beam.DoFn):\n",
        "    def process(self, element, creds):\n",
        "        from googleapiclient import discovery\n",
        "        from oauth2client.client import GoogleCredentials\n",
        "        import time\n",
        "        import logging\n",
        "        credentials = GoogleCredentials.get_application_default()\n",
        "        service = discovery.build(\"healthcare\", \"v1beta1\", credentials=credentials)\n",
        "        name = element[\"name\"]\n",
        "        request = service.projects().locations().datasets().operations().get(name=name)\n",
        "        while True:\n",
        "            response = request.execute()\n",
        "            result = response.json()\n",
        "            done = result.get(\"done\", \"\")\n",
        "            if done == True:\n",
        "                break\n",
        "            else:\n",
        "                print(\"operation is in progress, wait for 30 secs and recheck..\")\n",
        "                time.sleep(30)\n",
        "        if \"error\" in element:\n",
        "            error = element[\"error\"]\n",
        "            if error != None and error != \"\":\n",
        "                #sys.exit(-1)\n",
        "                logging.error(error)\n",
        "        return response\n",
        "\n",
        "# pcollection = {\"name\": \"<api response details>\"}"
      ],
      "metadata": {
        "id": "Bgd5idK-09az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BQ Append (Not worked)"
      ],
      "metadata": {
        "id": "WJ7ne-X3Frj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from apache_beam.io.gcp.bigquery import BigQueryDisposition\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.bigquery.schema import SchemaField\n",
        "\n",
        "\n",
        "def get_schema(table_id):\n",
        "  bq_client = bigquery.Client()\n",
        "  table = bq_client.get_table(table_id)\n",
        "  original_schema = table.schema\n",
        "  '''\n",
        "  for i in original_schema:\n",
        "    if not i.is_nullable:\n",
        "      print(i.name)\n",
        "  with open(\"original_schema.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(str(original_schema))\n",
        "  '''\n",
        "  replaced_schema = create_schema(original_schema)\n",
        "  return replaced_schema\n",
        "\n",
        "def create_schema(source_schema):\n",
        "  schema = []\n",
        "  for i in source_schema:\n",
        "    print(i.name, i.field_type, i.mode)\n",
        "    if i.mode == \"REPEATED\":\n",
        "      schemafield = SchemaField(i.name, i.field_type, mode=\"REPEATED\")\n",
        "    else:\n",
        "      schemafield = SchemaField(i.name, i.field_type, mode=\"NULLABLE\")\n",
        "    schema.append(schemafield)\n",
        "    if i.field_type == \"RECORD\":\n",
        "      schemafield._fields = create_schema(i.fields)\n",
        "  return str(schema)\n",
        "\n",
        "\n",
        " | \"Write to BQ\" >> beam.io.WriteToBigBigQuery(\n",
        "     table = <>,\n",
        "     dataset = <>,\n",
        "     project = <>,\n",
        "     schema = get_schema(project.dataset.table),\n",
        "     create_disposition=BigQueryDisposition.CREATE_IF_NEEDED,\n",
        "     write_disposition=BigQueryDisposition.WRITE_APPEND\n",
        ")"
      ],
      "metadata": {
        "id": "7aDHsKWqFrBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debug\n",
        "1. for **setup.py and extra package**, pls provide full path of the airflow location if you are calling beam pipeline from composer, else only specify relative path if you are calling beam pipeline directly without composer.\n",
        "2. Also **extra package**, takes the argument as list, so please check if you specifying multiple packages, ensure to add those inside the list.\n",
        "3. Also, make sure in **composer**, after dags folder, please upload empty **__init__.py** into each and every subfolders.\n",
        "4. etc.\n",
        "\n",
        "#### Folder Structure in Composer\n",
        ">\n",
        "> dags\n",
        ">\n",
        ">> composer_dag.py\n",
        ">\n",
        ">> dependent folder \n",
        ">\n",
        ">>> __init__.py\n",
        ">\n",
        ">>> setup.py\n",
        ">\n",
        ">>> beam_gcs_to_hc.py\n",
        ">\n",
        ">>> healthcare_lib\n",
        ">\n",
        ">>>> __init__.py\n",
        ">\n",
        ">>>> doc_lib.py\n",
        ">\n",
        ">>> dist\n",
        ">\n",
        ">>>> __init__.py\n",
        ">\n",
        ">>>> package-0.1.0.tar.gz"
      ],
      "metadata": {
        "id": "lxiKmPy6eCOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ref:\n",
        "#### **BeamRunPythonPipelineOperator**: \n",
        "> documentation: https://airflow.apache.org/docs/apache-airflow-providers-apache-beam/stable/_api/airflow/providers/apache/beam/operators/beam/index.html\n",
        "> \n",
        "> sample code: https://airflow.apache.org/docs/apache-airflow-providers-apache-beam/stable/operators.html"
      ],
      "metadata": {
        "id": "4dNr1pPJbY7F"
      }
    }
  ]
}
