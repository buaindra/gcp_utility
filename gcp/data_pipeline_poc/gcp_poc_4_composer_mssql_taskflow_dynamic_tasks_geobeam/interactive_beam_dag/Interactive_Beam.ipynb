{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAreeoc9eYwGIEMdOgjMf+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buaindra/gcp_utility/blob/main/gcp/data_pipeline_poc/gcp_poc_4_composer_mssql_taskflow_dynamic_tasks_geobeam/interactive_beam_dag/Interactive_Beam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ref\n",
        "1. Beam Doc: https://medium.com/@zahir.alward/part1-learning-apache-beam-programming-in-python-8f76ca80d1d0\n",
        "\n",
        "2. Beam: https://beam.apache.org/documentation/io/developing-io-overview/\n",
        "\n",
        "3. Beam SQL on Notebook: https://beam.apache.org/blog/beam-sql-with-notebooks/\n",
        "\n",
        "4. Beam Python SDK: https://beam.apache.org/releases/pydoc/2.43.0/\n",
        "\n",
        "5. Beam Programming guide: https://beam.apache.org/documentation/programming-guide/\n",
        "\n",
        "6. Codelab for panda df: https://codelabs.developers.google.com/codelabs/dataflow-notebooks-streamingwordcount#3\n",
        "\n",
        "7. (Aggregate) CombineGlobally:\n",
        "https://beam.apache.org/documentation/transforms/python/aggregation/combineglobally/"
      ],
      "metadata": {
        "id": "9vdllDKvRLnS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7d4RvqERKwO"
      },
      "outputs": [],
      "source": [
        "!pip install --user apache-beam[interactive] apache-beam[GCP]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "# get Length of the each element of the pcollection\n",
        "with beam.Pipeline() as p:\n",
        "    pcol = (p | beam.Create([\"Indra\", \"Shivam\"]) \n",
        "              | beam.Map(lambda x: x+\"_\"+str(len(x)))\n",
        "    )\n",
        "\n",
        "    pcol1 = (pcol.apply(beam.Map(print)))\n",
        "\n",
        "print(f\"pcol: {pcol}\") \n",
        "print (f\"type of pcol: {type(pcol)}\")"
      ],
      "metadata": {
        "id": "iV9H_G3jTgHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "notebook Service Account Should have below roles\n",
        "  1. Dataflow Worker\n",
        "  2. Bigquery Data Viewer (Dataset Level)\n",
        "  3. Bigquery User (Project Level)\n",
        "\"\"\"\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions \n",
        "from apache_beam.options.pipeline_options import SetupOptions, GoogleCloudOptions \n",
        "from apache_beam.runners.interactive import interactive_runner \n",
        "import apache_beam.runners.interactive.interactive_beam as ib\n",
        "from apache_beam.io.gcp.bigquery import ReadFromBigQuery \n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "bq_project_id = \"sample-sbx-toc\" \n",
        "bq_dataset = \"examples\"\n",
        "bq_table = \"FLD_HAZ_AR\"\n",
        "\n",
        "## Beam PTransform Function\n",
        "class CountPer_Column(beam.PTransform):\n",
        "    def init_(self, column): \n",
        "        self.column = column\n",
        "\n",
        "    def expand(self, pcol): \n",
        "        out_pcol = (pcol\n",
        "                    | f\"Add {self.column} as Key\" >> beam.Map(lambda x: (x.get(self.column, \"None\"), x))\n",
        "        )\n",
        "        return out_pcol | f\"Count per {self.column}\" >> beam.combiners.Count.PerKey()\n",
        "\n",
        "## Beam Pardo Function\n",
        "class Converted_Dict_To_Dataframe(beam.DoFn):\n",
        "    def process(self, element):\n",
        "    # print(f\"type of element: {type(element)}\") \n",
        "    ## convert JSON to DataFrame by using json_normalize(), read_json() and Dataframe.from_dict() \n",
        "    row_json_str = json.dumps(element) \n",
        "    row_df = pd.read_json(row_json_str, orient=\"index\")\n",
        "    yield row_df\n",
        "\n",
        "## python function for beam.Map \n",
        "def _convertValues(element):\n",
        "    if str(element[1]) == str(-9999.0):\n",
        "        return (element[0], \"0.0\")\n",
        "    else:\n",
        "        return (element[0], element[1])\n",
        "\n",
        "## python function for beam. CombinePerkey\n",
        "def _max(value_list):\n",
        "    float_values = [float(value) for value in value_list]\n",
        "    return max(float_values)\n",
        "\n",
        "##pipeline options not require for interactive beam \n",
        "pipeline_options = PipelineOptions()\n",
        "pipeline_options.view_as(SetupOptions).save_main_session = True\n",
        "\n",
        "## beam pipeline\n",
        "# pipeline = beam.Pipeline(interactive_runner.InteractiveRunner(), options=pipeline_options)\n",
        "pipeline = beam.Pipeline(interactive_runner.InteractiveRunner())\n",
        "pcol_bq_data = (pipeline \n",
        "                | \"Read From Bigquery\" >> ReadFromBigQuery(\n",
        "                      method = ReadFromBigQuery.Method.EXPORT,\n",
        "                      #table = f\"{bq_project_id}:{bq_dataset}.{ba table}\", \n",
        "                      project = \"{bq_project_id}\",\n",
        "                      dataset = \"{bq_dataset}\",\n",
        "                      query= \"\"\"SELECT * EXCEPT(geom) \n",
        "                      FROM `{bq_project_id}.{bq_dataset}.{bq_table}`; \n",
        "                      \"\"\",\n",
        "                      coder = beam.io.gcp.bigquery_read_internal._JsonToDictCoder, \n",
        "                      use_standard_sql = True, \n",
        "                      query_priority = beam.io.gcp.bigquery.BigQueryQueryPriority.BATCH, \n",
        "                      output_type=\"PYTHON_DICT\", \n",
        "                      gcs_location=\"gs://tmp_geobeam bucket_1/bq_gcs/\")\n",
        "                )\n",
        "\n",
        "## Combine Globally\n",
        "pcol_total_row_count = (pcol_bq_data | \"Total Row Count\" >> beam.combiners.Count.Globally())\n",
        "\n",
        "#Use beam.combiners.Count.Perkey() with PTransform\n",
        "pcol_count_per_SOURCE_CIT = (pcol_bq_data | CountPer_Column(column=\"SOURCE_CIT\"))\n",
        "\n",
        "## without PTransform\n",
        "pcol_count_per_ZONE_SUBTY = (pcol_bq_data \n",
        "                             | beam.Map(lambda x: (x.get(\"ZONE_SUBTY\", \"None\"), x.get(\"DEPTH\", 0))) \n",
        "                             | beam.Map(_convertValues)\n",
        "                            )\n",
        "\n",
        "## Use beam.CombinePerkey\n",
        "pcol_maxcount_per_ZONE_SUBTY = (pcol_count_per_ZONE_SUBTY | beam.CombinePerkey(_max))\n",
        "\n",
        "## converted to Pandas Dataframe\n",
        "pcol_df = (pcol_bq_data | \"Transform Each Row\" >> beam.Pardo(Converted_Dict_To_Dataframe()))\n",
        "\n",
        "\n",
        "result = pipeline.run()\n",
        "Job_status = result.wait_until_finish()\n",
        "\n",
        "print(f\"result: {result}\") # return interactive_runner.PipelineResult object\n",
        "print(f\"job_status: {job_status}\")  # return None"
      ],
      "metadata": {
        "id": "Y4aXqRL5Uhob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "import apache_beam.runners.interactive.interactive_beam as ib\n",
        "\n",
        "ib.show(pcol_total_row_count)\n",
        "ib.show(pcol_count_per_SOURCE_CIT)\n",
        "ib.show(pcol_count_per_ZONE_SUBTY.apply(beam.Filter(lambda x: x[0] != None)))\n",
        "ib.show(pcol_maxcount_per_ZONE_SUBTY)"
      ],
      "metadata": {
        "id": "97sWBVu1UhVp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}