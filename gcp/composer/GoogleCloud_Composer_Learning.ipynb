{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogleCloud_Composer_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMb9OUPEVmiMT1mn801q3W3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buaindra/gcp_utility/blob/main/gcp/composer/GoogleCloud_Composer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Airflow and Composer\n",
        "\n",
        "#### Airflow Summit 2022: https://www.crowdcast.io/e/airflowsummit2022/\n"
      ],
      "metadata": {
        "id": "oTdHbT2-K--J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Airflow Ref\n",
        "1. Airflow official doc: https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html\n",
        "2. Google doc: https://cloud.google.com/composer/docs/composer-2/trigger-dags"
      ],
      "metadata": {
        "id": "X0FfW3EMaCno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Airflow Components\n",
        "1. **A scheduler,** which handles both triggering scheduled workflows, and submitting Tasks to the executor to run.\n",
        "\n",
        "2. **An executor,** which handles running tasks. In the default Airflow installation, this runs everything inside the scheduler, but most production-suitable executors actually push task execution out to workers.\n",
        "\n",
        "3. **A webserver,** which presents a handy user interface to inspect, trigger and debug the behaviour of DAGs and tasks.\n",
        "\n",
        "4. **A folder of DAG files,** read by the scheduler and executor (and any workers the executor has)\n",
        "\n",
        "5. **A metadata database,** used by the scheduler, executor and webserver to store state.\n",
        "\n"
      ],
      "metadata": {
        "id": "4kEuCtcVp3O1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DAG\n",
        "> Directed Acyclic Graph\n",
        ">\n",
        "1. DAG Ref:\n",
        "  1. Airflow official doc: https://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html"
      ],
      "metadata": {
        "id": "DmffxztaSA5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dag Run\n",
        "1. Dag Run Status\n",
        "  1. success\n",
        "  2. failed\n",
        "  3. skipped\n",
        "\n",
        "2. DAG argument:\n",
        "  1. **schedule_interval**\n",
        "        1. it can be a cron expression as str, cron preset, datetime.timedelta(minutes=3)\n",
        "\n",
        "        2. **Data Interval**: \n",
        "        1. means if dag scheduled for **@daily**, then data interval start at midnight of each day and end at midnight of next day.\n",
        "        2.  **execution date/logical date**:\n",
        "          1. denotes the start of the data interval, not when dag is actually executed.\n",
        "  2. **start_date**:\n",
        "        1. it also points to same logical date.\n",
        "        1. DAG run will only be scheduled one interval after start_date.  \n",
        "  3. **catchup:**\n",
        "        1. the scheduler by default kick of a dag run for any data interval that has not been run since the last data interval. this concept is called Catchup\n",
        "        2. if catchup = False in DAG argument, then scheduler creates dag run only for latest interval.\n",
        "\n",
        "  4. **depends_on_past:**\n",
        "\n",
        "  5. **trigger_rule:**\n",
        "  \n",
        "  6. **default_args:** its default argument to every task. \"Owner\" argument must needed for every task, which can be pass through default_arg.\n"
      ],
      "metadata": {
        "id": "LTGlsjDsLBV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sample DAG code"
      ],
      "metadata": {
        "id": "XV1Eg7vwg-bL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nYikgiBK4VI"
      },
      "outputs": [],
      "source": [
        "# from airflow import DAG\n",
        "from airflow.models.dag import DAG  # DAG object to initiate the DAG\n",
        "from airflow.operators.bash import BashOperator  # operator to operate as task\n",
        "\n",
        "import datetime\n",
        "import pendulum\n",
        "\n",
        "dag = DAG(\n",
        "    \"tutorial\",  # dag_id: unique identifier of your dag\n",
        "    default_args={\n",
        "        \"owner\": \"Airflow\",\n",
        "        \"depends_on_past\": True,\n",
        "        \"retries\": 1\n",
        "        \"retry_delay\": datetime.timedelta(minutes=3)  # datetime.timedelta(days=1)\n",
        "    },\n",
        "    start_date=pendulum.datetime(2015, 12, 1, tz=\"UTC\"),\n",
        "    description=\"a simple tutorial dag\",\n",
        "    schedule_interval = \"@daily\",\n",
        "    catchup=False,\n",
        ")\n",
        "\n",
        "t1 = EmptyOperator(task_id=\"task\", dag=dag)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example above, if the DAG is picked up by the scheduler daemon on 2016-01-02 at 6 AM, (or from the command line), a single DAG Run will be created with a data between 2016-01-01 and 2016-01-02, and the next one will be created just after midnight on the morning of 2016-01-03 with a data interval between 2016-01-02 and 2016-01-03."
      ],
      "metadata": {
        "id": "NdmSyXr8oJu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.bash import BashOperator\n",
        "import datetime\n",
        "\n",
        "\n",
        "with DAG(\n",
        "    \"sample_dag\",\n",
        "    description=\"dample dag for learning\"\n",
        "    default_args={\n",
        "        \"owner\": \"Airflow\",\n",
        "    },\n",
        "    start_date=datetime.datetime.now(),\n",
        "    schedule_interval=datetime.timedelta(days=1),  \n",
        ") as dag:\n",
        "\n",
        "  t1 = BashOperator(\n",
        "      task_id=\"print_date\",\n",
        "      bash_command=\"date\",\n",
        "  )\n",
        "\n",
        "  t2 = BashOperator(\n",
        "      task_id='sleep',\n",
        "      depends_on_past=False,\n",
        "      bash_command='sleep 5',\n",
        "      retries=3,\n",
        "  )"
      ],
      "metadata": {
        "id": "BWng5cr7sJgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual tasks defined here will run in a different context from the context of this script. Different tasks run on different workers at different points in time, which means that this script cannot be used to cross communicate between tasks. Note that for this purpose we have a more advanced feature called **XComs**."
      ],
      "metadata": {
        "id": "8qSOrkdQoR2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XComs"
      ],
      "metadata": {
        "id": "q32qeTcYoSFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jF2joxhz-T5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Templating with Jinja\n",
        "#### Ref:\n",
        "1. Airflow template ref: https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html#templates-ref\n",
        "2. Jinja: https://jinja.palletsprojects.com/en/latest/api/#custom-filters \n",
        "\n",
        "### Uses:\n",
        "1. Jinja template only can be accessible on operators' templated field.\n",
        "2. python function also can accept jinja template\n",
        "3. use config values inside dag:\n",
        "```python\n",
        "var = \"{{ dag_run.conf['key'] }}\"\n",
        "```"
      ],
      "metadata": {
        "id": "A2-oHZR4oSUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "templated_command = dedent(\n",
        "    \"\"\"\n",
        "{% for i in range(5) %}\n",
        "    echo \"{{ ds }}\"\n",
        "    echo \"{{ macros.ds_add(ds, 7)}}\"\n",
        "{% endfor %}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "t3 = BashOperator(\n",
        "    task_id='templated',\n",
        "    depends_on_past=False,\n",
        "    bash_command=templated_command,\n",
        ")"
      ],
      "metadata": {
        "id": "QaglfH7eoT7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_this_func(**context):\n",
        "    \"\"\"\n",
        "    Print the payload \"message\" passed to the DagRun conf attribute.\n",
        "    :param context: The execution context\n",
        "    :type context: dict\n",
        "    \"\"\"\n",
        "    print(\"context\", context)\n",
        "    print(\"Remotely received value of {} for key=message\".format(context[\"dag_run\"].conf[\"key\"]))\n",
        "\n",
        "#PythonOperator usage\n",
        "run_this = PythonOperator(task_id=\"run_this\", python_callable=run_this_func, dag=dag, provide_context=True)\n"
      ],
      "metadata": {
        "id": "a0auYajpFA2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BashOperator usage\n",
        "bash_task = BashOperator(\n",
        "    task_id=\"bash_task\",\n",
        "    bash_command='echo \"Here is the message: \\'{{ dag_run.conf[\"key\"] if dag_run else \"\" }}\\'\"',\n",
        "    dag=dag\n",
        ")"
      ],
      "metadata": {
        "id": "0eKosApVoT3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SparkSubmitOperator usage\n",
        "spark_task = SparkSubmitOperator(\n",
        "        task_id=\"task_id\",\n",
        "        conn_id=spark_conn_id,\n",
        "        name=\"task_name\",\n",
        "        application=\"example.py\",\n",
        "        application_args=[\n",
        "            '--key', '\\'{{ dag_run.conf[\"key\"] if dag_run else \"\" }}\\''\n",
        "        ],\n",
        "        num_executors=10,\n",
        "        executor_cores=5,\n",
        "        executor_memory='30G',\n",
        "        #driver_memory='2G',\n",
        "        conf={'spark.yarn.maxAppAttempts': 1},\n",
        "        dag=dag)"
      ],
      "metadata": {
        "id": "JXJqZq0CoT1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sensors\n",
        "##### ref: https://www.youtube.com/watch?v=fgm3BZ3Ubnw"
      ],
      "metadata": {
        "id": "iVIuqMaUOUTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow.sensors.filesystem import FileSensor\n",
        "\n",
        "waiting_for_file = FileSensor(\n",
        "    task_id=\"waiting_for_file\",\n",
        "    poke_interval=30,  # in every 30 secs, sensor will check for the file\n",
        "    timeout= 60 * 5,  # 5 mints timeout, best practice to use to avoid deadlock. else your task will work on worker and wait for file and will not be finished.\n",
        "    mode=\"reschedule\",  # default is \"poke\". reschedule helps sensor to release the worker during the interval time so other task can use that worker.\n",
        "    soft_fail=True  # default is False. If its true and execution time is greater than sensor timeout, then it will skip the task.\n",
        ")"
      ],
      "metadata": {
        "id": "MRug2RkXO345"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ExternalTaskSensor\n",
        "##### Ref: https://www.youtube.com/watch?v=Oemg-3aiAiI"
      ],
      "metadata": {
        "id": "DoDVDHYFOWWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group tasks inside DAGs"
      ],
      "metadata": {
        "id": "0yidf7ZwTcqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "goLpIUxJoTxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5-3MBpwIoTvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GsvgSoyEoTqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "q4yJAEAJoToV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fBQ7261LoThu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SjhRyAJToTfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QT4wOoQGoTO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ov6NYkw1oSaU"
      }
    }
  ]
}