{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogleCloud_Composer_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOIMkXDEjiQvd+OZlBrEMU1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buaindra/gcp_utility/blob/main/gcp/composer/GoogleCloud_Composer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Airflow and Composer\n",
        "\n",
        "#### Airflow Summit 2022: https://www.crowdcast.io/e/airflowsummit2022/\n"
      ],
      "metadata": {
        "id": "oTdHbT2-K--J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Airflow Ref\n",
        "1. Airflow official doc: https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html\n",
        "1. Airflow Concepts: https://airflow.apache.org/docs/apache-airflow/1.10.4/concepts.html\n",
        "2. Google doc: https://cloud.google.com/composer/docs/composer-2/trigger-dags\n",
        "\n",
        "### Try blogs:\n",
        "1. Medium blog: https://github.com/mbrukman/gcp-pso/blob/master/examples/cloud-composer-examples/composer_dataflow_examples/simple_load_dag.py"
      ],
      "metadata": {
        "id": "X0FfW3EMaCno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Airflow Components\n",
        "1. **A scheduler,** which handles both triggering scheduled workflows, and submitting Tasks to the executor to run.\n",
        "\n",
        "2. **An executor,** which handles running tasks. In the default Airflow installation, this runs everything inside the scheduler, but most production-suitable executors actually push task execution out to workers.\n",
        "\n",
        "3. **A webserver,** which presents a handy user interface to inspect, trigger and debug the behaviour of DAGs and tasks.\n",
        "\n",
        "4. **A folder of DAG files,** read by the scheduler and executor (and any workers the executor has)\n",
        "\n",
        "5. **A metadata database,** used by the scheduler, executor and webserver to store state.\n",
        "\n"
      ],
      "metadata": {
        "id": "4kEuCtcVp3O1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DAG\n",
        "> Directed Acyclic Graph\n",
        ">\n",
        "1. DAG Ref:\n",
        "  1. Airflow official doc: https://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html"
      ],
      "metadata": {
        "id": "DmffxztaSA5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dag Run\n",
        "1. Dag Run Status\n",
        "  1. success\n",
        "  2. failed\n",
        "  3. skipped\n",
        "\n",
        "2. DAG argument:\n",
        "  1. **schedule_interval**\n",
        "        1. it can be a cron expression as str, cron preset, datetime.timedelta(minutes=3)\n",
        "\n",
        "        2. **Data Interval**: \n",
        "        1. means if dag scheduled for **@daily**, then data interval start at midnight of each day and end at midnight of next day.\n",
        "        2.  **execution date/logical date**:\n",
        "          1. denotes the start of the data interval, not when dag is actually executed.\n",
        "  2. **start_date**:\n",
        "        1. it also points to same logical date.\n",
        "        1. DAG run will only be scheduled one interval after start_date.  \n",
        "  3. **catchup:**\n",
        "        1. the scheduler by default kick of a dag run for any data interval that has not been run since the last data interval. this concept is called Catchup\n",
        "        2. if catchup = False in DAG argument, then scheduler creates dag run only for latest interval.\n",
        "\n",
        "  4. **depends_on_past:**\n",
        "        1. keep it False, then if earlier task has been failed will not impact the next time task execution.\n",
        "\n",
        "  5. **trigger_rule:**\n",
        "        1. ALL_SUCCESS = 'all_success'\n",
        "        2. ALL_FAILED = 'all_failed'\n",
        "        3. ALL_DONE = 'all_done'\n",
        "        4. ONE_SUCCESS = 'one_success'\n",
        "        5. ONE_FAILED = 'one_failed'\n",
        "  \n",
        "  6. **default_args:** its default argument to every task. \"Owner\" argument must needed for every task, which can be pass through default_arg.\n"
      ],
      "metadata": {
        "id": "LTGlsjDsLBV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hook, Sensonsors and Operators\n",
        "1. Operators: In composer or airflow dag, we are mostly using Operators, each operator creates individual task in airflow dag.\n",
        "like: PythonOperator, BashOperator\n",
        "\n",
        "2. Hook: Hooks are interfaces to services external to the airflow cluster. While operators provide a way to create tasks that may or may not communicate with some external service, hooks provide a uniform interface to access external services like S3, Postgres, MYSQL, Hive etc.\n",
        "\n",
        "3. Sensors: Sensors are special kind of operator that are designed to wait for something to happen."
      ],
      "metadata": {
        "id": "o6mX1lf9j15i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ref:\n",
        "https://towardsdatascience.com/apache-airflow-tips-and-best-practices-ff64ce92ef8\n",
        "\n",
        "### Execution Time\n",
        "1. The execution time is not the actual run time, but rather the start timestamp of its schedule period.\n",
        "2. The execution time of manual triggered DAG, would be exactly when it was triggered.\n",
        "\n",
        "### start_date:\n",
        "\n",
        "### schedule_interval:"
      ],
      "metadata": {
        "id": "HUi72IqKKMSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sample DAG code"
      ],
      "metadata": {
        "id": "XV1Eg7vwg-bL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nYikgiBK4VI"
      },
      "outputs": [],
      "source": [
        "# from airflow import DAG\n",
        "from airflow.models.dag import DAG  # DAG object to initiate the DAG\n",
        "from airflow.operators.bash import BashOperator  # operator to operate as task\n",
        "\n",
        "import datetime\n",
        "import pendulum\n",
        "\n",
        "dag = DAG(\n",
        "    \"tutorial\",  # dag_id: unique identifier of your dag\n",
        "    default_args={\n",
        "        \"owner\": \"Airflow\",\n",
        "        \"depends_on_past\": True,\n",
        "        \"retries\": 1\n",
        "        \"retry_delay\": datetime.timedelta(minutes=3)  # datetime.timedelta(days=1)\n",
        "    },\n",
        "    start_date=pendulum.datetime(2015, 12, 1, tz=\"UTC\"),\n",
        "    description=\"a simple tutorial dag\",\n",
        "    schedule_interval = \"@daily\",\n",
        "    catchup=False,\n",
        ")\n",
        "\n",
        "t1 = EmptyOperator(task_id=\"task\", dag=dag)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example above, if the DAG is picked up by the scheduler daemon on 2016-01-02 at 6 AM, (or from the command line), a single DAG Run will be created with a data between 2016-01-01 and 2016-01-02, and the next one will be created just after midnight on the morning of 2016-01-03 with a data interval between 2016-01-02 and 2016-01-03."
      ],
      "metadata": {
        "id": "NdmSyXr8oJu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.bash import BashOperator\n",
        "import datetime\n",
        "\n",
        "\n",
        "with DAG(\n",
        "    \"sample_dag\",\n",
        "    description=\"dample dag for learning\"\n",
        "    default_args={\n",
        "        \"owner\": \"Airflow\",\n",
        "    },\n",
        "    start_date=datetime.datetime.now(),\n",
        "    schedule_interval=datetime.timedelta(days=1),  \n",
        ") as dag:\n",
        "\n",
        "  t1 = BashOperator(\n",
        "      task_id=\"print_date\",\n",
        "      bash_command=\"date\",\n",
        "  )\n",
        "\n",
        "  t2 = BashOperator(\n",
        "      task_id='sleep',\n",
        "      depends_on_past=False,\n",
        "      bash_command='sleep 5',\n",
        "      retries=3,\n",
        "  )"
      ],
      "metadata": {
        "id": "BWng5cr7sJgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual tasks defined here will run in a different context from the context of this script. Different tasks run on different workers at different points in time, which means that this script cannot be used to cross communicate between tasks. Note that for this purpose we have a more advanced feature called **XComs**."
      ],
      "metadata": {
        "id": "8qSOrkdQoR2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XComs\n",
        "\n",
        "#### Ref: https://www.youtube.com/watch?v=8veO7-SN5ZY\n",
        "\n",
        "1. cross communinication/share data between tasks\n",
        "2. stored inside airflow metadata database\n",
        "3. python finction returns data which is by default xcom with key: \"return_value\".\n",
        "4. every operator in airflow, push xcom\n",
        "4. in Python function:\n",
        "  1. use \"ti\" as parameter to python function for task_instance object\n",
        "  2. then push xcom by using ti.xcom_push(key=\"key\", value=\"value\")\n",
        "  3. then pull xcom by using ti.xcom_pull(key=\"key\", task_ids=\\<task_id\\>)\n",
        "5. in bash operator, last print statement, push x_com by default. To avaoid that you can use, \"do_xcom_push=False\"\n",
        "\n"
      ],
      "metadata": {
        "id": "q32qeTcYoSFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dags/xcom_dag.py\n",
        "\n",
        "from airflow import DAG\n",
        "from airflow.operators.bash import BashOperator\n",
        "from airflow.operators.python import PythonOperator\n",
        "#from airflow.operators.python_operator import PythonOperator\n",
        "from airflow.operators.dummy import DummyOperator\n",
        "from airflow.models.param import Param\n",
        "from airflow.models import Variable\n",
        "\n",
        "from random import uniform\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "\n",
        "project_id = Variable.get(\"project_id\")\n",
        "region = eval(Variable.get(\"region\", default_var=\"us-central1\"))\n",
        "\n",
        "default_args = {\n",
        "    \"depends_on_past\": False,\n",
        "    \"project_id\": project_id,\n",
        "    \"region\": region,\n",
        "    \"start_date\": datetime(2020, 1, 1),\n",
        "    \"retries\": 1,\n",
        "    \"retry_delay\": timedelta(minutes=3)\n",
        "}\n",
        "\n",
        "def _training_model(ti):\n",
        "    accuracy = uniform(0.1, 10.0)\n",
        "    print(f'model\\'s accuracy: {accuracy}')\n",
        "    ti.xcom_push(key='model_accuracy', value=accuracy)\n",
        "\n",
        "def _choose_best_model(**kwargs):\n",
        "    ti = kwargs['ti']  # another way to access ti\n",
        "    print('choose best model')\n",
        "    accuracies = ti.xcom_pull(key='model_accuracy', task_ids=['training_model_A', 'training_model_B', 'training_model_C'])\n",
        "    print(accuracies)\n",
        "\n",
        "with DAG(dag_id='xcom_dag', \n",
        "         schedule_interval='@daily', \n",
        "         default_args=default_args, \n",
        "         catchup=False,\n",
        "         tags=[\"ingestion\"]\n",
        "         params = {\n",
        "             'SRC_TBL_NM': Param(\"ALL\", type=[\"string\", \"array\"]),\n",
        "         },\n",
        "    ) as dag:\n",
        "\n",
        "    downloading_data = BashOperator(\n",
        "        task_id='downloading_data',\n",
        "        bash_command='sleep 3',\n",
        "        do_xcom_push=False\n",
        "    )\n",
        "\n",
        "    # check below one..\n",
        "    training_model_task = [\n",
        "        PythonOperator(\n",
        "            task_id=f'training_model_{task}',\n",
        "            python_callable=_training_model\n",
        "        ) for task in ['A', 'B', 'C']]\n",
        "\n",
        "    choose_model = PythonOperator(\n",
        "        task_id='choose_model',\n",
        "        python_callable=_choose_best_model,\n",
        "        provide_context=True,  # all default variable will be passed as key-value argument\n",
        "        #op_kwargs={\"key\": \"{{ params.SRC_TBL_NM }}\"}\n",
        "    )\n",
        "\n",
        "    downloading_data >> training_model_task >> choose_model"
      ],
      "metadata": {
        "id": "daa3VxxeR-aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BashOperator"
      ],
      "metadata": {
        "id": "P-ipNoggMB3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow.operators.bash_operator import BashOperator\n",
        "\n",
        "load_file_to_bq = BashOperator(\n",
        "    task_id=\"load_file_to_bq\",\n",
        "    bash_command = \"bq load --autodetect --source_format=CSV --field_delimiter ',' project:dataset.table_name gs://bucket/prefix/blob_name\",\n",
        "    dag=dag\n",
        ")"
      ],
      "metadata": {
        "id": "m6S4n-RHL9RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variables on Airflow\n",
        "##### Ref: https://airflow.apache.org/docs/apache-airflow/stable/concepts/variables.html\n",
        "\n",
        "1. Stored on Airflow metadata database\n",
        "\n",
        "1. Variables are Airflow’s runtime configuration concept - a general key/value store that is global and can be queried from your tasks, and easily set via Airflow’s user interface, or bulk-uploaded as a JSON file.\n",
        "\n",
        "2. Variables are global, and should only be used for overall configuration that covers the entire installation; to pass data from one Task/Operator to another, you should use XComs instead."
      ],
      "metadata": {
        "id": "98LTL1SVL2SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow.models import Variable\n",
        "\n",
        "dict_val = \"foo\"\n",
        "Variable.set(\"foo\", dict_val)\n",
        "foo = Variable.get(\"foo\")\n"
      ],
      "metadata": {
        "id": "hsYOc91fL23_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also use them from templates:\n",
        "# Raw value\n",
        "echo {{ var.value.<variable_name> }}\n",
        "\n",
        "# Auto-deserialize JSON value\n",
        "echo {{ var.json.<variable_name> }}"
      ],
      "metadata": {
        "id": "SaIMhcD3MEkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PythonOperator\n",
        "\n",
        "#### Ref: \n",
        "1. https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html\n",
        "2. https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html\n",
        "3. https://airflow.apache.org/docs/apache-airflow/1.10.4/howto/operator/python.html"
      ],
      "metadata": {
        "id": "_fhAa3n1ZZBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BranchPythonOperator\n",
        "\n"
      ],
      "metadata": {
        "id": "K1qai18efCi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow.operators.python import BranchPythonOperator\n",
        "\n",
        "def _branching_func(**kwargs):\n",
        "    ti = kwargs[ti]\n",
        "    executable_tasks = ti.xcom_pull(key=\"final_table_name\", task_ids=\"read_config_db\")\n",
        "    new_list = [\"tk1_\"+str(table) for table in executable_tasks]\n",
        "    return new_list\n",
        "\n",
        "read_config_db = PythonOperator(...)\n",
        "\n",
        "branch_task = BranchPythonOperator(\n",
        "    task_id=\"branch_task\",\n",
        "    provide_context=True,\n",
        "    pyhton_callable=_branching_func,\n",
        "    do_xcom_push=False,\n",
        ")\n",
        "\n",
        "def return_config():\n",
        "    table_list = [\"table_1\", \"table_2\"]\n",
        "    return table_list\n",
        "\n",
        "read_config_db >> branch_task   \\\n",
        ">> [DummyOperator(task_id=f\"tk1_{table}\", dag=dag) for table in return_config()]  \\\n",
        ">> end"
      ],
      "metadata": {
        "id": "uv3RSxAdfDJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SubDag"
      ],
      "metadata": {
        "id": "J2AqdIOihSfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mRf3AzBEhWoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Templating with Jinja\n",
        "#### Ref:\n",
        "1. Airflow template ref: https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html#templates-ref\n",
        "2. Jinja: https://jinja.palletsprojects.com/en/latest/api/#custom-filters \n",
        "\n",
        "### Uses:\n",
        "1. Jinja template only can be accessible on operators' templated field.\n",
        "2. python function also can accept jinja template\n",
        "3. use config values inside dag:\n",
        "```python\n",
        "var = \"{{ dag_run.conf['key'] }}\"\n",
        "```"
      ],
      "metadata": {
        "id": "A2-oHZR4oSUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "templated_command = dedent(\n",
        "    \"\"\"\n",
        "{% for i in range(5) %}\n",
        "    echo \"{{ ds }}\"\n",
        "    echo \"{{ macros.ds_add(ds, 7)}}\"\n",
        "{% endfor %}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "t3 = BashOperator(\n",
        "    task_id='templated',\n",
        "    depends_on_past=False,\n",
        "    bash_command=templated_command,\n",
        ")"
      ],
      "metadata": {
        "id": "QaglfH7eoT7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_this_func(**context):\n",
        "    \"\"\"\n",
        "    Print the payload \"message\" passed to the DagRun conf attribute.\n",
        "    :param context: The execution context\n",
        "    :type context: dict\n",
        "    \"\"\"\n",
        "    print(\"context\", context)\n",
        "    print(\"Remotely received value of {} for key=message\".format(context[\"dag_run\"].conf[\"key\"]))\n",
        "\n",
        "#PythonOperator usage\n",
        "run_this = PythonOperator(task_id=\"run_this\", python_callable=run_this_func, dag=dag, provide_context=True)\n"
      ],
      "metadata": {
        "id": "a0auYajpFA2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BashOperator usage\n",
        "bash_task = BashOperator(\n",
        "    task_id=\"bash_task\",\n",
        "    bash_command='echo \"Here is the message: \\'{{ dag_run.conf[\"key\"] if dag_run else \"\" }}\\'\"',\n",
        "    dag=dag\n",
        ")"
      ],
      "metadata": {
        "id": "0eKosApVoT3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SparkSubmitOperator usage\n",
        "spark_task = SparkSubmitOperator(\n",
        "        task_id=\"task_id\",\n",
        "        conn_id=spark_conn_id,\n",
        "        name=\"task_name\",\n",
        "        application=\"example.py\",\n",
        "        application_args=[\n",
        "            '--key', '\\'{{ dag_run.conf[\"key\"] if dag_run else \"\" }}\\''\n",
        "        ],\n",
        "        num_executors=10,\n",
        "        executor_cores=5,\n",
        "        executor_memory='30G',\n",
        "        #driver_memory='2G',\n",
        "        conf={'spark.yarn.maxAppAttempts': 1},\n",
        "        dag=dag)"
      ],
      "metadata": {
        "id": "JXJqZq0CoT1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sensors\n",
        "##### ref: https://www.youtube.com/watch?v=fgm3BZ3Ubnw"
      ],
      "metadata": {
        "id": "iVIuqMaUOUTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow.sensors.filesystem import FileSensor\n",
        "\n",
        "waiting_for_file = FileSensor(\n",
        "    task_id=\"waiting_for_file\",\n",
        "    poke_interval=30,  # in every 30 secs, sensor will check for the file\n",
        "    timeout= 60 * 5,  # 5 mints timeout, best practice to use to avoid deadlock. else your task will work on worker and wait for file and will not be finished.\n",
        "    mode=\"reschedule\",  # default is \"poke\". reschedule helps sensor to release the worker during the interval time so other task can use that worker.\n",
        "    soft_fail=True  # default is False. If its true and execution time is greater than sensor timeout, then it will skip the task.\n",
        ")"
      ],
      "metadata": {
        "id": "MRug2RkXO345"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow.providers.google.cloud.sensors.gcs import GCSObjectExistenceSensor\n",
        "\n",
        "check_gcs_file_exists_or_not = GCSObjectExistenceSensor(\n",
        "    task_id = \"check_gcs_file_exists_or_not\",\n",
        "    bucket = \"bucket_name\",\n",
        "    object = \"blob_name\",\n",
        "    # schedule_interval=60,\n",
        "    mode=\"reschedule\",\n",
        "    poke_interval = 120\n",
        ")"
      ],
      "metadata": {
        "id": "Dqu0WicOKtPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ExternalTaskSensor\n",
        "##### Ref: https://www.youtube.com/watch?v=Oemg-3aiAiI\n",
        "\n",
        "##### Hints:\n",
        "1. Not recommend to use\n",
        "2. Should have same execution time for parent dag and child dag, else need to specify delta time parameter\n",
        "3. both parent and child dag should have same schedule interval."
      ],
      "metadata": {
        "id": "DoDVDHYFOWWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow.sensors.extarnal_task import ExternalTaskSensor\n",
        "\n",
        "extarnal_sensor = ExternalTaskSensor(\n",
        "    task_id=\"extarnal_sensor\",\n",
        "    external_dag_id=\"child_dag\",\n",
        "    external_task_id=\"child_task\",\n",
        "    timeout=600,\n",
        "    mode=\"reschedule\",\n",
        ")"
      ],
      "metadata": {
        "id": "-f55LXYpdpqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TriggerDagRunOperator\n",
        "\n",
        "##### Hints:\n",
        "1. ** Need to check execution dag time for child dag\n",
        "2. By default, Trigger dag will not wait for the completion status of target date. Default is, wait_for_completion=False\n"
      ],
      "metadata": {
        "id": "IhBHV_Ldk_lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow.operators.trigger_dagrun import TriggerDagRunOperator\n",
        "\n",
        "trigger_child_dag = TriggerDagRunOperator(\n",
        "    task_id=\"trigger_child_dag\",\n",
        "    trigger_dag_id=\"child_dag\",\n",
        "    execution_date=\"{{ ds }}\",  # target dag will take current execution date of the trigger dag\n",
        "    reset_dag_run=True,  # used for backfilling, \n",
        "    wait_for_completion=True,  # trigger dag will wait untill target dag has not been completed\n",
        "    poke_interval=10,  # in every 10 sec it will check target dag completed or not\n",
        "    dag=dag\n",
        ")"
      ],
      "metadata": {
        "id": "b4c74RTtlAM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group tasks inside DAGs"
      ],
      "metadata": {
        "id": "0yidf7ZwTcqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from  airflow.utils.task_group import TaskGroup"
      ],
      "metadata": {
        "id": "Wfrn-hHZkhLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic Task creation"
      ],
      "metadata": {
        "id": "Ov6NYkw1oSaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if return_config():\n",
        "    for table in return_config():  # return_config() will return table list\n",
        "        with TaskGroup(group_id=f\"group_{table}\") as tg1:\n",
        "            t1 = DummyOperator(task_id=f\"t1_{table}\", dag=dag)\n",
        "            t2 = DummyOperator(task_id=f\"t2_{table}\", dag=dag)\n",
        "\n",
        "            [read_config_db >> t1 >> t2 >> end]\n",
        "\n",
        "start >> read_config_db >> tg1 >> end"
      ],
      "metadata": {
        "id": "KY2Eg5Olis_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic Task Mapping in Airflow 2.3"
      ],
      "metadata": {
        "id": "-Ar-o-NditbN"
      }
    }
  ]
}