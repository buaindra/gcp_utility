{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Azure_Databricks_Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPgF5uQ7WZp6VHIgAahOd0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buaindra/gcp_utility/blob/main/bigdata/Azure_Databricks_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Azure Databricks and Spark\n",
        "### Ref:\n",
        "1. Coursera Course: https://www.coursera.org/learn/perform-data-science-with-azure-databricks/lecture/Wn6zD/explain-azure-databricks\n",
        "2. Coursera Course: https://www.coursera.org/learn/big-data-integration-processing/home/week/1"
      ],
      "metadata": {
        "id": "erPC0ptSFVfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Engineering evolution\n",
        "1. Types of Data:\n",
        "  1. Sturctured Data (RDMS: MSSQL, Oracle, Postgres, MySQL)\n",
        "  2. Semi-Structured Data (Json, XML)\n",
        "  3. Un-Structured Data (files, Images, Videos)\n",
        "\n",
        "2. Bigdata Criteria:\n",
        "  1. Scalability\n",
        "    1. **Vertical scalling**: singale machine got higher CPU, Memory, Disk Storage and its vertically increase the single system. Its complex and time taking and old fashioned. \n",
        "    2. **Horizontal scalling**: multiple same machines added into the network. For distributed approch, its so faster to scale up and down horizontally.\n",
        "  2. **Fault-tolerance** and **High-availibility (HA)**\n",
        "  3. **Cost-Effectiveness**\n",
        "\n",
        "3. Bigdata solution approach:\n",
        "  1. Monolithic: One Standalone, Large and Robust system, to handle bigdata (Teradata, Exadata). Supports structured and semi-structured.\n",
        "  2. Distributed: Takes many smaller clusters to store data distributed way and process the data parallel manner.\n"
      ],
      "metadata": {
        "id": "aazBBRPnO2-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hadoop\n",
        "**Apache Hadoop** is an open source framework that is used to efficiently store and process large datasets ranging in size from gigabytes to petabytes of data. Instead of using one large computer to store and process the data, Hadoop allows clustering multiple computers to analyze massive datasets in parallel more quickly.\n",
        "\n",
        "> Distributed computing and parallel processing\n",
        "\n"
      ],
      "metadata": {
        "id": "-yAnQ8-hABzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hadoop consists of four main core modules:\n",
        "1. Hadoop Distributed File System (HDFS) –\n",
        "  > A distributed file system that runs on standard or low-end hardware. HDFS provides better data throughput than traditional file systems, in addition to high fault tolerance and native support of large datasets.\n",
        "\n",
        "  > Component:\n",
        "    1. Name Node (NN):\n",
        "      1. NN keeping the records of file metadata (file name, size, directory location, file blocks, block id, block location etc.)\n",
        "    2. Data Node (DN):\n",
        "      1. Typically, in **Hadoop 2.0 Block size is 128 MB**.\n",
        "\n",
        "2. Yet Another Resource Negotiator (YARN) –\n",
        "  > Cluster Resourse Manager\n",
        "\n",
        "  > Manages and monitors cluster nodes and resource usage. It schedules jobs and tasks.\n",
        "\n",
        "  > Main Component:\n",
        "    1. RM: Resourse Manager\n",
        "    2. NM: Node Manager\n",
        "    3. AM: Application Master (*Need to check more*)\n",
        "      1. RM tells any one of the NM to create AM container, inside AM container single and individual application is being executing.\n",
        "       \n",
        "\n",
        "3. MapReduce – \n",
        "  > A framework that helps programs do the parallel computation on data. The map task takes input data and converts it into a dataset that can be computed in key value pairs. The output of the map task is consumed by reduce tasks to aggregate output and provide the desired result.\n",
        "\n",
        "  > Consepts:\n",
        "    1. Programming Model \n",
        "      1. Way of programing concepts to solve the problem.\n",
        "    2. Programming Framework\n",
        "      1. Consists of set of api, services.\n",
        "\n",
        "4. Hadoop Common – \n",
        "  > Provides common Java libraries that can be used across all modules.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QkIib8dNKPZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create directory in hadoop\n",
        "hadoop fs -mkdir /user/data\n",
        "\n",
        "# copy files from local to hadoop\n",
        "hadoop fs -copyFromLocal sample.txt /user/data/\n",
        "\n",
        "# list files in hadoop\n",
        "hadoop fs -ls /user/data/"
      ],
      "metadata": {
        "id": "ubN_r_vskGnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### On top of this Hadoop Core Systems there are other hadoop services:\n",
        "1. Hive: Relational database and accept simple sql query inside in it.\n",
        "2. HBase: NoSql database to perform large volumn dataset.\n",
        "3. Scoop: Ingestion Tool for import/export data to/from HDFS. (*Need to check more*)\n",
        "4. Oozie: mostly ETL tool. (*Need to check more*)\n",
        "5. Pig: Language similar like python. (*Need to check more*)\n",
        "6. Scala: Language similar like unix/python. (*Need to check more*)\n",
        "7. Fortron: Databricks SQL Analytics\n"
      ],
      "metadata": {
        "id": "LJd3lqlO_79-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scenarios in Hadoop development:\n",
        "\n",
        "1. Application Master Container:\n",
        "  1. One data processing program (Hadoop M/R program) = One AM Container + Many other mapper and reducers. It follows master/slave architecture. So AM container is the master and the other mapper/reducers are the slaves. Always remember, you can have only one master. So only one AM container for one application.\n",
        "\n",
        "2. If Map lost or failed:\n",
        "  1. Lost mapper\n",
        "    1. The mapper sends a heartbeat to the AM. If the AM doesn't receive a heartbeat, we assume the task failed or lost and the AM will start a new mapper for doing the work. \n",
        "\n",
        "  2. Failed mapper\n",
        "    1. If the mapper failed with some runtime exception, the JVM notifies the error to the AM and the AM marks the mapper as failed. We also have some configurations to retry failed mappers. So AM might retry the mapper depending on the configuration. But when all the retries failed, the application fails with an error/exception."
      ],
      "metadata": {
        "id": "HD5qNYATkXFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Lake and Data Warehouse basic concept:\n",
        "1. Data Lake\n",
        "  1. is a blob storage, where you can store structured, non-structured, semi-structured , any kind of data, files.\n",
        "  2. example: HDFS, S3, GCS, Azure Blob Storage, Cassendra File System.\n",
        "\n",
        "2. Data Warehouse \n",
        "  1. is a OLAP database, supports only structured data.\n",
        "  2. example: teradata, exadata.\n"
      ],
      "metadata": {
        "id": "FWeqwc2bmPQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Spark?\n",
        "Spark is data processing framework which workes on top of the distributed file system, cluster nodes and Resource Managers (Yarn, Mesos, Kubernates)\n",
        "\n",
        "1. Spark is a data processing platform.\n",
        "2. Spark Application architecture:\n",
        "  1. Spark Driver (1):\n",
        "  2. Spark Executor (n):\n",
        "2. There are 2 ways to process the data in Spark:\n",
        "  1. **spark table/database and sql**\n",
        "    1. **physical data storage layer**: actual data store in backend in a file\n",
        "        1. normal db has .dba file, only support structured data\n",
        "        2. spark support avro, csv, parquet file, any types of file, unstructured or semi structured data and distributed storage (s3, hdfs, ADLS) as well.\n",
        "\n",
        "    2. **logical data layer**: have row and column display of data.\n",
        "        1. its also compute layer, runs on spark sql engine (combined of spark sql and spark dataframe api)\n",
        "\n",
        "    3. **metadata layer**: consists of table schema, table creation date \n",
        "and other info.\n",
        "        1. Meta store for spark sql, its based on hive metastore/mysql(~10GB)\n",
        "        2. Catalog for spark dataframe\n",
        "        3. spark on hadoop stores metadata on hive datastore using **hive metadata catalog api**.\n",
        "\n",
        "  2. **Spark dataframe**\n",
        "    1. spark df is similar like spark table without metadata store.\n",
        "    2. it has runtime metadata store which is called **catalog**.\n",
        "    3. spark df is runtime memory object  and its also got deleted once application finished.\n",
        "    4. **A distributed collection of data grouped into named columns**.\n",
        "    5. **Dataframe Methods**:\n",
        "        1. Actions: Its trigger spark job, return to the **spark driver**.\n",
        "          1. Example: Collect, show, Describe, first, head, tail, foreach, summary etc.(total 12)\n",
        "        2. Transformations: produces newly transformed dataframe\n",
        "          1. Example: filter, groupby, join, union (total 38)\n",
        "        3. Function/methods:other than actions and transforms.\n",
        "          1. Example: CreateorReplaceTempView etc (total 20)\n",
        "  \n",
        "  3. Differences between Spark table and dataframe:\n",
        "    1. **Spark Table is persistent objects, so it can be accessed across all the sessions/notebooks.**\n",
        "    2. **Spark Dataframe is session specific, so if you created dataframe in 1 session/notebook, can't be accessed from another session/notebook.**\n",
        "     \n",
        "  4. **Spark Submit Tool**:\n",
        "    1. how to migrate local spark code (initial script, library files, config files) to cluster.\n",
        "    2. spark application runs with 1 driver (starts inside AM ) and multiple executors.\n",
        "    3. options:\n",
        "        1. deploy-mode:\n",
        "          1. Client: driver will not create on cluster , it will create on edge node as an independent java application.\n",
        "          2. Cluster: not default value but recommended.\n",
        "  \n",
        "  5. **SparkSession**:\n",
        "  > 1 workspace = 1 application can have multiple spark sessions (notebooks) but will have only 1 spark context\n",
        "  >\n",
        "    1. Attributes: \n",
        "        1. builder: builder methods:\n",
        "            1. appname\n",
        "            2.getorCreate\n",
        "\n",
        "        1. read, readstream\n",
        "        2. conf\n",
        "        3. catalog\n",
        "\n",
        "        4. **Spark Context**:\n",
        "          1. a spark context represents the connection to a spark cluster and can use to create RDD and broadcast variables on the cluster.\n",
        "          2. is a gateway to connect spark core api.\n",
        "\n",
        "  6. **Spark UI**:\n",
        "    1. jobs, \n",
        "    2. stages\n",
        "    3. storage\n",
        "    3. environment\n",
        "    4. executors\n"
      ],
      "metadata": {
        "id": "B7EiGI40uqRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# single spark code\n",
        "spark-submit --master yarn  --deploy-mode client wordcount.py /user/data/sample.txt\n",
        "\n",
        "# spark code with supporting lib and config files\n",
        "spark-submit --master yarn  --deploy-mode cluster -py-files lib.zip --fileslog4j.properties, spark.conf --conf 'spark.driver.extraJavaOption=-Dlog4j.configuration=log4j.properties,'  wordcount.py /user/data/sample.txt"
      ],
      "metadata": {
        "id": "CYWu2osJlFNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Key Notes:\n",
        "1. Spark is **100x times faster** than Map-Reduce. Its actually do parallel jobs on distributed data.\n",
        "2. Spark can be easily worked with Python, Scala, Java, R, **ease of use**.\n",
        "3. Sparks **combines SQL, streaming, complex analytics** (Machine Learning)\n",
        "4. Spark **can run anywhare** (Apache Hadoop, Apache Mesos, Databricks, Kubernates, Standalone Cluster etc.)\n",
        "5. *spark dataframe and pandas dataframes* are not similar. **Dataframe** is a data structure and inside it we can perform various operations.\n",
        "6. Spark Solution:\n",
        "  1. Spark with Hadoop: Datalake\n",
        "  2. Spark without hadoop: lakehouse\n",
        "  \n",
        "7. create **edge_node** as extra machine beyong master and worker node, as its not good practice to connect directly master/worker nodes. So developer can connect edge_node to access/store spark code .\n",
        "  1. upload code file to edge_node\n",
        "  2. upload data file to storage layer\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RwJ8-jjoKk09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spark RDD vs Dataframes \n",
        "1. RDD: Resilient Distributed Dataset\n",
        "1. Spark recommend to use Dataframe now. Spark Dataframe is a lazy evaluation on top of RDD.\n",
        "2. Dataframe is also diributed like RDD, and Dataframe is more optimized.\n",
        "3. but RDD is schemaless, datafram will create schema."
      ],
      "metadata": {
        "id": "TIHfVHgZw6j6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = pyspark.SparkContext()\n",
        "rdd = sc.parallelize([1,1,2,3,4,5])\n",
        "rdd.distinct().count()"
      ],
      "metadata": {
        "id": "hr3MSwbKRb6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_list = [1, 1, 2, 3, 4, 6, 5, 3, 8]\n",
        "sc = spark.sparkContext.parallelize(sample_list)\n",
        "print(type(sc))\n",
        "\n",
        "sc.distinct().count()"
      ],
      "metadata": {
        "id": "d43knyaXX4t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Spark Dataframe\n",
        "1. count is action or transformation?\n",
        "  1. DataFrame.count() -> Action\n",
        "  2. GroupedData.count() -> Transformation\n",
        "\n",
        "2. how to create spark dataframe:\n",
        "  1. read\n",
        "    ```python\n",
        "    df1 = spark.read \\\n",
        "            .format(\"csv\") \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .option(\"inferSchema\", \"true\") \\\n",
        "            .load(\"sample.txt)\n",
        "    ```\n",
        "  2. sql()\n",
        "    ```python\n",
        "    df2 = spark.sql(\"\"\"\n",
        "    select count(distinct calltype) as distinct_call_type from fire_service_call_view where calltype is not NULL\n",
        "    \"\"\")\n",
        "    ```\n",
        "  3. table()\n",
        "    ```python\n",
        "    df3 = spark.table(\"spark_db_name.table_name\")\n",
        "    ```\n",
        "  4. range(): create single column dataframe\n",
        "    ```python\n",
        "    df4 = spark.range(5)\n",
        "    ```\n",
        "  5. createDataFrame(): converts a list (python list, Spark Row, panda dataframe, RDD) into a spark dataframe\n",
        "    ```python\n",
        "    df1 = spark.createDataFrame(data_list)\n",
        "    ```"
      ],
      "metadata": {
        "id": "r4oNigARZIB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Spark SQL\n",
        "1. Cache Table:\n",
        "  1. It will create temporary view on top of the table.\n",
        "```sql\n",
        "cache lazy table fire_service_calls_tbl_cache as\n",
        "select * from demo_db.fire_service_calls_tbl\n",
        "```\n",
        "  2. if table has been cached, you can use view or table name, spark always use cache."
      ],
      "metadata": {
        "id": "N83q7304c01o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Databricks\n",
        "1. Databricks is a ETL tools. It supports spark.\n",
        "2. Databricks uses optimized spark on top of apache spark which is 5 times faster than apache spark.\n",
        "3. Databricks only available on cloud (Azure, GCP, AWS).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KWKyDthzGJWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Databricks Community Login\n",
        "1. Ref: https://community.cloud.databricks.com/login.html\n",
        "\n",
        "#### Databricks Workspace menu:\n",
        "1. Workspace Type:\n",
        "  1. Data Science & Engineering:\n",
        "  2. Machine Learning: \n",
        "\n",
        "2. Compute\n",
        "  1. All-purpose clusters: suitable for dev and test work.\n",
        "  2. Job cluster: not available for community edition and its similar with ephemeral gcp cluster (when job finished, cluster will be shut down).\n",
        "  3. **databricks community delete (cluster VM, spark metastore) the cluster after 2 hours of inactivity. But they don't delete the spark sql storage layer. So you will get error while creating Spark Table again.**\n",
        "\n",
        "3. Jobs:\n",
        "  1. this option is also not available from community edition\n",
        "\n",
        "4. Settings:\n",
        "  1. user settings:\n",
        "    1. you can add git links, change passwords.\n",
        "  2. Admin Console:\n",
        "    1. you can add users.\n",
        "    2. you can provide global init (shell script) to install anything on cluster before spark run and which are not installed via databricks runtime.\n",
        "\n",
        "5. Create:\n",
        "  1. Notebook\n",
        "  2. table\n",
        "  3. cluster"
      ],
      "metadata": {
        "id": "bcjcXeUNPwLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delta Lake\n",
        "  1. Its provide ACID transaction and data consistency feature.\n",
        "  2. ref:\n",
        "    1. https://docs.databricks.com/delta/delta-intro.html\n",
        "  2. **How to improve data processing in databricks**:\n",
        "    1. blogs from Databricks: https://databricks.com/blog/2022/05/20/five-simple-steps-for-implementing-a-star-schema-in-databricks-with-delta-lake.html\n",
        "\n",
        "  3. provide acid property. (google datastream, azure synapsys etc.)"
      ],
      "metadata": {
        "id": "dKVBLPmB9x5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PySpark Ref:\n",
        "1. Youtube: \n",
        "  1. https://www.youtube.com/watch?v=_C8kWso4ne4&t=597s\n",
        "2. Spark Official Doc:\n",
        "  1. Python API Ref: https://spark.apache.org/docs/latest/api/python/reference/index.html\n"
      ],
      "metadata": {
        "id": "upru3DSMGMUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PySpark:\n",
        "1. *PySpark is an interface for Apache Spark in Python*, is often used for large scale data.\n",
        "2. create and start **SparkSession** before writing pyspark\n",
        "3. when using **inferSchema=True** while read data from csv, please check the file size, as if inferSchema enabled, its reads the whole file once and provide the datatype of columns based on the data, otherwise, *by default all columns will be string*.\n",
        "4. Check columns datatype or statistics on spark dataframe\n",
        "  1. **printSchema(), dtypes, describe()**\n",
        "5. check: **show()** and **collect()**\n",
        "6. check: **select()** and **withcolumns()**\n",
        "7. check: **where and filter**\n",
        "7. check: **sort and orderby**\n",
        "7. check: **na.drop**, **na.fill**\n",
        "7. check: **display**\n",
        "8. check: **describe extended \\<table name\\>, show tables, spark.catalog.listTables()**\n",
        "9. "
      ],
      "metadata": {
        "id": "Seb-_dRoAQRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install PySpark"
      ],
      "metadata": {
        "id": "rAcuQOdkUVvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark\n",
        "\n",
        "# Successfully installed py4j-0.10.9.3 pyspark-3.2.1"
      ],
      "metadata": {
        "id": "vBPxt0tzSNlx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb91b7e7-e089-445d-c376-604325814cda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/py4j/\u001b[0m\n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 52.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=9d9d326ea65f899261b843ca8674d2f9be89ccafc572ca2fee4306a5a547d24d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Understand Pandas Dataframe"
      ],
      "metadata": {
        "id": "ZzZgDugkY84o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/sample_emp.csv\")\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU_M6u1nW_S1",
        "outputId": "b0abe649-547f-44d4-b01a-2c28d0471633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Name   age  Experience   Salary\n",
            "0      Krish  31.0        10.0  30000.0\n",
            "1  Sudhanshu  30.0         8.0  25000.0\n",
            "2      Sunny  29.0         4.0  20000.0\n",
            "3       Paul  24.0         3.0  20000.0\n",
            "4     Harsha  21.0         1.0  15000.0\n",
            "5    Shubham  23.0         2.0  18000.0\n",
            "6     Mahesh   NaN         NaN  40000.0\n",
            "7        NaN  34.0        10.0  38000.0\n",
            "8        NaN  36.0         NaN      NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to start writing PySpark"
      ],
      "metadata": {
        "id": "CaSSOD4cZB5H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUZ7Vl9GFOEt"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#spark = SparkSession.builder.appName(\"Practise\").getOrCreate()\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SparkSession"
      ],
      "metadata": {
        "id": "ywQ7qNIgGQQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# debug SparkSession variable\n",
        "spark\n",
        "# print(spark.getActiveSession)\n",
        "# print(spark.version)\n",
        "# print(spark.conf)\n",
        "# print(spark.sparkContext)\n",
        "# print(spark._instantiatedSession)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "8ZSCGhB-GMdo",
        "outputId": "b057c8e9-a6be-4c22-d037-fe85ec2a948f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fc5d540a4d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4aa31c1a7845:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to read csv file in pyspark"
      ],
      "metadata": {
        "id": "6e8hwoB_FCWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diamonds_df = spark.read.format(\"csv\") \\\n",
        "                .option(\"header\", \"true\") \\\n",
        "                .option(\"inferSchema\", \"true\") \\\n",
        "                .load(\"/content/diamonds.csv\")\n",
        "\n",
        "diamonds_df.show(10)"
      ],
      "metadata": {
        "id": "WweUxatsukMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ps = spark.read.csv(\"/content/sample_emp.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "XwEeOtDXYvnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fetch the data from spark dataframs"
      ],
      "metadata": {
        "id": "45ol6UWhGB0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display all dataframe rows and columns\n",
        "print(df_ps.show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg-6FhwMFpmt",
        "outputId": "bd0303f8-9c98-4575-f552-1d2c05e05591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+----------+------+\n",
            "|     Name| age|Experience|Salary|\n",
            "+---------+----+----------+------+\n",
            "|    Krish|  31|        10| 30000|\n",
            "|Sudhanshu|  30|         8| 25000|\n",
            "|    Sunny|  29|         4| 20000|\n",
            "|     Paul|  24|         3| 20000|\n",
            "|   Harsha|  21|         1| 15000|\n",
            "|  Shubham|  23|         2| 18000|\n",
            "|   Mahesh|null|      null| 40000|\n",
            "|     null|  34|        10| 38000|\n",
            "|     null|  36|      null|  null|\n",
            "+---------+----+----------+------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display 2 records from top\n",
        "print(df_ps.show(2))  # similar with head(2) as its also showing top 2 records \n",
        "print(df_ps.head(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWduuTqbKbwi",
        "outputId": "54c4816c-9a55-4a02-e48f-95ee9f2fc8fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+----------+------+\n",
            "|     Name|age|Experience|Salary|\n",
            "+---------+---+----------+------+\n",
            "|    Krish| 31|        10| 30000|\n",
            "|Sudhanshu| 30|         8| 25000|\n",
            "+---------+---+----------+------+\n",
            "only showing top 2 rows\n",
            "\n",
            "None\n",
            "[Row(Name='Krish', age=31, Experience=10, Salary=30000), Row(Name='Sudhanshu', age=30, Experience=8, Salary=25000)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display 2 records from down\n",
        "print(df_ps.tail(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVFJCmccKhYc",
        "outputId": "0a32cca6-f05a-4752-8732-0f38c86dbbf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(Name=None, age=34, Experience=10, Salary=38000), Row(Name=None, age=36, Experience=None, Salary=None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check datatypes of dataframe\n",
        "1. using **printSchema()** method\n",
        "2. or using **dtypes** property\n"
      ],
      "metadata": {
        "id": "1IymH9nqY4sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display the dataframe schema\n",
        "# in pandas its, df.info()\n",
        "print(df_ps.printSchema())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlVZk3fGLNcn",
        "outputId": "5b2d117a-8b19-4397-e758-23f999bf073e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- Experience: integer (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ps.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwLHm0LGZA3t",
        "outputId": "fdc48e65-d05e-485f-bc5e-bf497baf591d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Name', 'string'), ('age', 'int'), ('Experience', 'int'), ('Salary', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Computes basic statistics for numeric and string columns.\n",
        "1. using **describe()**"
      ],
      "metadata": {
        "id": "oRsLbMfVffyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_ps.describe())\n",
        "\n",
        "print(df_ps.describe().show())\n",
        "\n",
        "print(df_ps.describe(\"age\").show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qg_u3xsd5GY",
        "outputId": "6286f35b-363a-4924-8b16-5321c7c8bec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[summary: string, Name: string, age: string, Experience: string, Salary: string]\n",
            "+-------+------+------------------+------------------+-----------------+\n",
            "|summary|  Name|               age|        Experience|           Salary|\n",
            "+-------+------+------------------+------------------+-----------------+\n",
            "|  count|     7|                 8|                 7|                8|\n",
            "|   mean|  null|              28.5| 5.428571428571429|          25750.0|\n",
            "| stddev|  null|5.3718844791323335|3.8234863173611093|9361.776388210581|\n",
            "|    min|Harsha|                21|                 1|            15000|\n",
            "|    max| Sunny|                36|                10|            40000|\n",
            "+-------+------+------------------+------------------+-----------------+\n",
            "\n",
            "None\n",
            "+-------+------------------+\n",
            "|summary|               age|\n",
            "+-------+------------------+\n",
            "|  count|                 8|\n",
            "|   mean|              28.5|\n",
            "| stddev|5.3718844791323335|\n",
            "|    min|                21|\n",
            "|    max|                36|\n",
            "+-------+------------------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display the dataframe columns\n",
        "print(df_ps.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NSiXKsQLNbI",
        "outputId": "1d009c09-3376-490b-a563-65692b1bc720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Name', 'age', 'Experience', 'Salary']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Differebce between show() and collect()\n",
        "\n",
        "\n",
        "|id | show() | collect()|\n",
        "|--- | --- | ---|\n",
        "|1 | Returns None, display the rows and columns as tabular format | Returns  all the records as list|\n",
        "\n",
        "\n",
        "```python\n",
        "# similar \n",
        "df_ps.select([\"Name\", \"age\"]).show()\n",
        "df_ps.select(\"Name\", \"age\").show()\n",
        "\n",
        "df_ps.select([\"Name\", \"age\"]).collect()\n",
        "df_ps.select(\"Name\", \"age\").collect()\n",
        "```"
      ],
      "metadata": {
        "id": "b6AvEtq2RBOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display specific columns and custom columns from dataframes\n",
        "\n",
        "df_ps_retirement_yr = df_ps.select(\"Name\", \"age\", (60-df_ps.age).alias(\"retirement_yr_remaining\")).collect() # create list of rows\n",
        "print(df_ps_retirement_yr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_97kx6kLNU0",
        "outputId": "d2176663-09c7-43a3-8b88-ef9c50f013d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(Name='Krish', age=31, retirement_yr_remaining=29), Row(Name='Sudhanshu', age=30, retirement_yr_remaining=30), Row(Name='Sunny', age=29, retirement_yr_remaining=31), Row(Name='Paul', age=24, retirement_yr_remaining=36), Row(Name='Harsha', age=21, retirement_yr_remaining=39), Row(Name='Shubham', age=23, retirement_yr_remaining=37), Row(Name='Mahesh', age=None, retirement_yr_remaining=None), Row(Name=None, age=34, retirement_yr_remaining=26), Row(Name=None, age=36, retirement_yr_remaining=24)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to add new/custom columns into dataframe using **select()** and **withcolumns()**\n",
        "#### Differences betwen select() and withcolumns()\n",
        "\n",
        "| id | select() | withcolumns() |\n",
        "|---|---|---|\n",
        "|1| select is used for to select specific columns | withColumns is used to add a new custom column|\n"
      ],
      "metadata": {
        "id": "1ZT4PSlhhPUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_select = df_ps.select(\"name\",  \"age\", (60-df_ps.age).alias(\"retirement_yr_remaining\"))\n",
        "print(df_out_select.show())\n",
        "\n",
        "df_out_withcolumn = df_ps.withColumn(\"retirement_yr_remaining\", 60-df_ps.age)\n",
        "# df_out_withcolumn = df_ps.withColumn(\"retirement_yr_remaining\", 60-df_ps[\"age\"])  # df_ps.age and df_ps[\"age\"] are same\n",
        "print(df_out_withcolumn.show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3lWoAH4Sm7m",
        "outputId": "87b739a7-d597-446e-c456-dacd7025e812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+-----------------------+\n",
            "|     name| age|retirement_yr_remaining|\n",
            "+---------+----+-----------------------+\n",
            "|    Krish|  31|                     29|\n",
            "|Sudhanshu|  30|                     30|\n",
            "|    Sunny|  29|                     31|\n",
            "|     Paul|  24|                     36|\n",
            "|   Harsha|  21|                     39|\n",
            "|  Shubham|  23|                     37|\n",
            "|   Mahesh|null|                   null|\n",
            "|     null|  34|                     26|\n",
            "|     null|  36|                     24|\n",
            "+---------+----+-----------------------+\n",
            "\n",
            "None\n",
            "+---------+----+----------+------+-----------------------+\n",
            "|     Name| age|Experience|Salary|retirement_yr_remaining|\n",
            "+---------+----+----------+------+-----------------------+\n",
            "|    Krish|  31|        10| 30000|                     29|\n",
            "|Sudhanshu|  30|         8| 25000|                     30|\n",
            "|    Sunny|  29|         4| 20000|                     31|\n",
            "|     Paul|  24|         3| 20000|                     36|\n",
            "|   Harsha|  21|         1| 15000|                     39|\n",
            "|  Shubham|  23|         2| 18000|                     37|\n",
            "|   Mahesh|null|      null| 40000|                   null|\n",
            "|     null|  34|        10| 38000|                     26|\n",
            "|     null|  36|      null|  null|                     24|\n",
            "+---------+----+----------+------+-----------------------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to drop columns from pyspark dataframes"
      ],
      "metadata": {
        "id": "F8p_xWEHoad1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_out = df_out_withcolumn.drop(\"retirement_yr_remaining\")\n",
        "print(\"After dropping column\", df_out.show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTUxh6UNLNOj",
        "outputId": "48268eb8-1186-441a-cd5b-02eaaae58a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+----------+------+\n",
            "|     Name| age|Experience|Salary|\n",
            "+---------+----+----------+------+\n",
            "|    Krish|  31|        10| 30000|\n",
            "|Sudhanshu|  30|         8| 25000|\n",
            "|    Sunny|  29|         4| 20000|\n",
            "|     Paul|  24|         3| 20000|\n",
            "|   Harsha|  21|         1| 15000|\n",
            "|  Shubham|  23|         2| 18000|\n",
            "|   Mahesh|null|      null| 40000|\n",
            "|     null|  34|        10| 38000|\n",
            "|     null|  36|      null|  null|\n",
            "+---------+----+----------+------+\n",
            "\n",
            "After dropping column None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to rename a column into Dataframe"
      ],
      "metadata": {
        "id": "wejKFty-ph_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_renamed = df_out.withColumnRenamed(\"Age\",\"Actual Age\")\n",
        "print(df_out.show())  # no changes reflected on original dataframe\n",
        "print(df_out_renamed.show())  # changes reflected on returned dataframe after withColumnRenamed()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRH_WoLwLNJB",
        "outputId": "d32aee4a-fe3f-4897-d5ad-a4973f7a36c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+----------+------+\n",
            "|     Name| age|Experience|Salary|\n",
            "+---------+----+----------+------+\n",
            "|    Krish|  31|        10| 30000|\n",
            "|Sudhanshu|  30|         8| 25000|\n",
            "|    Sunny|  29|         4| 20000|\n",
            "|     Paul|  24|         3| 20000|\n",
            "|   Harsha|  21|         1| 15000|\n",
            "|  Shubham|  23|         2| 18000|\n",
            "|   Mahesh|null|      null| 40000|\n",
            "|     null|  34|        10| 38000|\n",
            "|     null|  36|      null|  null|\n",
            "+---------+----+----------+------+\n",
            "\n",
            "None\n",
            "+---------+----------+----------+------+\n",
            "|     Name|Actual Age|Experience|Salary|\n",
            "+---------+----------+----------+------+\n",
            "|    Krish|        31|        10| 30000|\n",
            "|Sudhanshu|        30|         8| 25000|\n",
            "|    Sunny|        29|         4| 20000|\n",
            "|     Paul|        24|         3| 20000|\n",
            "|   Harsha|        21|         1| 15000|\n",
            "|  Shubham|        23|         2| 18000|\n",
            "|   Mahesh|      null|      null| 40000|\n",
            "|     null|        34|        10| 38000|\n",
            "|     null|        36|      null|  null|\n",
            "+---------+----------+----------+------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### how to drop rows which has null values\n",
        "\n",
        "##### drop parameters:\n",
        "1. *how* : str, optional\n",
        "  1. 'any' or 'all'.\n",
        "  2. If 'any', drop a row if it contains any nulls.\n",
        "  3. If 'all', drop a row only if all its values are null.\n",
        "\n",
        "2. *thresh*: int, optional\n",
        "  1. default None\n",
        "  2. If specified, drop rows that have less than thresh non-null values.\n",
        "  3. This overwrites the how parameter.\n",
        "\n",
        "3. *subset*: str, tuple or list, optional\n",
        "  1. optional list of column names to consider having null will be deleted."
      ],
      "metadata": {
        "id": "tkKQ3fyktbuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_out_renamed.na.drop(how=\"any\", thresh=1).show())\n",
        "print(df_out_renamed.na.drop(how=\"any\", subset=[\"Experience\"]).show())\n",
        "print(df_out_renamed.na.drop().show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyX3NydALM-Y",
        "outputId": "89d37ef0-c70c-44cd-fbd7-8295405e3c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+----------+------+\n",
            "|     Name|Actual Age|Experience|Salary|\n",
            "+---------+----------+----------+------+\n",
            "|    Krish|        31|        10| 30000|\n",
            "|Sudhanshu|        30|         8| 25000|\n",
            "|    Sunny|        29|         4| 20000|\n",
            "|     Paul|        24|         3| 20000|\n",
            "|   Harsha|        21|         1| 15000|\n",
            "|  Shubham|        23|         2| 18000|\n",
            "|   Mahesh|      null|      null| 40000|\n",
            "|     null|        34|        10| 38000|\n",
            "|     null|        36|      null|  null|\n",
            "+---------+----------+----------+------+\n",
            "\n",
            "None\n",
            "+---------+----------+----------+------+\n",
            "|     Name|Actual Age|Experience|Salary|\n",
            "+---------+----------+----------+------+\n",
            "|    Krish|        31|        10| 30000|\n",
            "|Sudhanshu|        30|         8| 25000|\n",
            "|    Sunny|        29|         4| 20000|\n",
            "|     Paul|        24|         3| 20000|\n",
            "|   Harsha|        21|         1| 15000|\n",
            "|  Shubham|        23|         2| 18000|\n",
            "|     null|        34|        10| 38000|\n",
            "+---------+----------+----------+------+\n",
            "\n",
            "None\n",
            "+---------+----------+----------+------+\n",
            "|     Name|Actual Age|Experience|Salary|\n",
            "+---------+----------+----------+------+\n",
            "|    Krish|        31|        10| 30000|\n",
            "|Sudhanshu|        30|         8| 25000|\n",
            "|    Sunny|        29|         4| 20000|\n",
            "|     Paul|        24|         3| 20000|\n",
            "|   Harsha|        21|         1| 15000|\n",
            "|  Shubham|        23|         2| 18000|\n",
            "+---------+----------+----------+------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to Transform the data"
      ],
      "metadata": {
        "id": "9xpJzZKsMkK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, to_timestamp, round\n",
        "\n",
        "fire_df = renamed_fire_df \\\n",
        "            .withColumn(\"CallDate\", to_date(\"CallDate\", \"MM/dd/yyyy\"))  \\\n",
        "            .withColumn(\"WatchDate\", to_date(\"WatchDate\", \"MM/dd/yyyy\"))  \\\n",
        "            .withColumn(\"AvailableDtTm\", to_timestamp(\"AvailableDtTm\", \"MM/dd/yyyy hh:mm:ss a\"))  \\\n",
        "            .withColumn(\"Delay\", round(\"Delay\", 2))"
      ],
      "metadata": {
        "id": "fhohiCYaMikg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specify Spark Dataframe Schema:\n",
        "1. DDL\n",
        "2. Struct type\n",
        "3. Notes:\n",
        "  1. Schema Inference is not advisable\n",
        "  1. often date, timestamp, boolean datatype can't store correct value into dataframe. Spark dataframe expect date in yyyy-mm-dd format normally. So if the data not loaded properly , it can store null without generating error. So add **mode** option.\n",
        "  2. **mode**: FAILFAST, DROPMALFORMED, PERMISSIVE\n",
        "  3. if there are 2 date columns, one is dd-mm-yyyy and another one is yyyy-mm-dd, then simply load the data as string datatype."
      ],
      "metadata": {
        "id": "jDPRHudYyIRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema_1 = [\"a\",\"b\",\"c\",\"d\",\"e\"] \n",
        "schema_2 = \"a int, b double, c string, d date, e timestamp\""
      ],
      "metadata": {
        "id": "1A-Hj5zlGZJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flight_schema_ddl = \"\"\"FL_DATE date, OP_CARRIER STRING, OP_CARRIER_FL_NUM INT, ORIGIN STRING, \n",
        "             ORIGIN_CITY_NAME STRING, DEST STRING, DEST_CITY_NAME STRING, CRS_DEP_TIME INT, DEP_TIME INT, \n",
        "             WHEELS_ON INT, TAXI_IN INT, CRS_ARR_TIME INT, ARR_TIME INT, CANCELLED STRING, DISTANCE INT\"\"\""
      ],
      "metadata": {
        "id": "AdhVyVTDyJJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flight_time_raw_df = spark.read \\\n",
        "                        .format(\"json\") \\\n",
        "                        .schema(flight_schema_ddl) \\\n",
        "                        .option(\"mode\", \"FAILFAST\") \\\n",
        "                        .option(\"dateFormat\", \"M/d/y\") \\\n",
        "                        .load(\"/FileStore/tables/flight_time.json\")"
      ],
      "metadata": {
        "id": "b9uP9toOxcIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "flight_schema_struct = StructType([\n",
        "    StructField(\"FL_Date\", DateType()),\n",
        "    StructField(\"Op_carrier\", StringType()),\n",
        "    StructField(\"Op_carrier_FL_Name\", IntegerType()),\n",
        "])"
      ],
      "metadata": {
        "id": "yDgBWnyWz1lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to fill/handle missing values (null) in dataframe"
      ],
      "metadata": {
        "id": "NTDptUDxyA31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_out_renamed.na.fill(\"missing\", subset=[\"Name\", \"Actual Age\",\"Experience\"]).show())  # doesn't affect \"Actual Age\",\"Experience\" as their datatype is int\n",
        "print(df_out_renamed.na.fill({'name': 'missing', 'actual age': 0, 'salary': 0.00}).show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqFcqze2yAgn",
        "outputId": "34bf340c-c6bd-4aa7-f975-b704787a909b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+----------+------+\n",
            "|     Name|Actual Age|Experience|Salary|\n",
            "+---------+----------+----------+------+\n",
            "|    Krish|        31|        10| 30000|\n",
            "|Sudhanshu|        30|         8| 25000|\n",
            "|    Sunny|        29|         4| 20000|\n",
            "|     Paul|        24|         3| 20000|\n",
            "|   Harsha|        21|         1| 15000|\n",
            "|  Shubham|        23|         2| 18000|\n",
            "|   Mahesh|      null|      null| 40000|\n",
            "|  missing|        34|        10| 38000|\n",
            "|  missing|        36|      null|  null|\n",
            "+---------+----------+----------+------+\n",
            "\n",
            "None\n",
            "+---------+----------+----------+------+\n",
            "|     Name|Actual Age|Experience|Salary|\n",
            "+---------+----------+----------+------+\n",
            "|    Krish|        31|        10| 30000|\n",
            "|Sudhanshu|        30|         8| 25000|\n",
            "|    Sunny|        29|         4| 20000|\n",
            "|     Paul|        24|         3| 20000|\n",
            "|   Harsha|        21|         1| 15000|\n",
            "|  Shubham|        23|         2| 18000|\n",
            "|   Mahesh|         0|      null| 40000|\n",
            "|  missing|        34|        10| 38000|\n",
            "|  missing|        36|      null|     0|\n",
            "+---------+----------+----------+------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imputer (\\** *Need to learn more about* )\n",
        "1. Imputer also helps in handling null values in spark dataframe by Mean, Median values."
      ],
      "metadata": {
        "id": "FYEunEA_ARM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "imputer = Imputer(\n",
        "    inputCols=['age', 'Experience', 'Salary'], \n",
        "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
        "    ).setStrategy(\"median\")\n",
        "\n",
        "# Add imputation cols to df\n",
        "imputer.fit(df_ps).transform(df_ps).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zhk4Ac8yAfD",
        "outputId": "6cccd788-6c1b-4f3c-a4ea-ea091fdbf0c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+----------+------+-----------+------------------+--------------+\n",
            "|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
            "+---------+----+----------+------+-----------+------------------+--------------+\n",
            "|    Krish|  31|        10| 30000|         31|                10|         30000|\n",
            "|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n",
            "|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
            "|     Paul|  24|         3| 20000|         24|                 3|         20000|\n",
            "|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
            "|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
            "|   Mahesh|null|      null| 40000|         29|                 4|         40000|\n",
            "|     null|  34|        10| 38000|         34|                10|         38000|\n",
            "|     null|  36|      null|  null|         36|                 4|         20000|\n",
            "+---------+----+----------+------+-----------+------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to filter the spark dataframe\n",
        "1. if you want to add multiple filter, separate the each filter using ()\n",
        "2. use \"~\" as not, \"|\" as or , \"&\" as and"
      ],
      "metadata": {
        "id": "JJVOddU8TSLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df_ps.filter(\"age > 21\").show())\n",
        "# print(df_ps.filter(df_ps.age > 21, ).show())\n",
        "print(df_ps.filter(df_ps[\"age\"] > 21).show())  # recommend to use, others also same in nature\n",
        "# print(df_ps.where(\"age > 21\").show())\n",
        "# print(df_ps.where(df_ps.age > 21, ).show())\n",
        "# print(df_ps.where(df_ps[\"age\"] > 21).show())"
      ],
      "metadata": {
        "id": "EVFKGnsEyAbx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61ce3bf6-bb3a-4ad8-a4c9-bf5028278ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+----------+------+\n",
            "|     Name|age|Experience|Salary|\n",
            "+---------+---+----------+------+\n",
            "|    Krish| 31|        10| 30000|\n",
            "|Sudhanshu| 30|         8| 25000|\n",
            "|    Sunny| 29|         4| 20000|\n",
            "|     Paul| 24|         3| 20000|\n",
            "|  Shubham| 23|         2| 18000|\n",
            "|     null| 34|        10| 38000|\n",
            "|     null| 36|      null|  null|\n",
            "+---------+---+----------+------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### use where and selcet same time on spark Dataframe similar like sql "
      ],
      "metadata": {
        "id": "5RZrykLUoiqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ps.where((df_ps[\"Salary\"] >= 20000) & (df_ps[\"Salary\"] < 35000)).select(df_ps[\"Name\"], df_ps[\"Age\"], df_ps[\"Salary\"]).show()\n",
        "# df_ps.where(df_ps[\"Salary\"] >= 20000).where(df_ps[\"Salary\"] < 35000).select(df_ps[\"Name\"], df_ps[\"Age\"], df_ps[\"Salary\"]).show()"
      ],
      "metadata": {
        "id": "RGn0FeaCyAZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cee6fb9-6f54-4d70-dd62-589128926dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+------+\n",
            "|     Name|Age|Salary|\n",
            "+---------+---+------+\n",
            "|    Krish| 31| 30000|\n",
            "|Sudhanshu| 30| 25000|\n",
            "|    Sunny| 29| 20000|\n",
            "|     Paul| 24| 20000|\n",
            "+---------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### get the data from the spark dataframe using groupBy"
      ],
      "metadata": {
        "id": "xbcJgAzHuOVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "results_df = diamonds_df.select(\"color\", \"price\") \\\n",
        "              .groupBy(\"color\") \\\n",
        "              .agg(avg(\"price\")) \\\n",
        "              .sort(\"color\")\n",
        "\n",
        "results_df.show()"
      ],
      "metadata": {
        "id": "O53prD0cuTLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fire_df.select(\"CallType\")  \\\n",
        "    .where(\"CallType is not null\")  \\\n",
        "    .groupBy(\"CallType\") \\\n",
        "    .count() \\\n",
        "    .orderBy(\"count\", ascending=False)"
      ],
      "metadata": {
        "id": "OHzBGt7vIjH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create catch\n",
        "Cached the table in-memory for faster fetching/quering on that table\n",
        "> lazy:\n",
        "> \n",
        "> lazy will execute the cache statement very fast as it will not cache the data immediately,\n",
        "it will mark the table to be cached.\n",
        "Instead of, at the 1st time of use of the table, it will cache the whole table data into memory.\n"
      ],
      "metadata": {
        "id": "_rjwX1jbK1ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fire_df.cache()\n",
        "\n",
        "cache lazy table fire_service_calls_tbl_cache as\n",
        "select * from demo_db.fire_service_calls_tbl"
      ],
      "metadata": {
        "id": "9YK5GjQpK1Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### link a temp view with this spark dataframe for sql query"
      ],
      "metadata": {
        "id": "F_vr9mhNkDOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diamonds_df.createOrReplaceTempView(\"tbl_diamonds\")\n",
        "diamonds_df = spark.sql(\"select * from tbl_diamonds where price <= 350\")\n",
        "diamonds_df.show()\n",
        "spark.catalog.dropTempView(\"tbl_diamonds\")"
      ],
      "metadata": {
        "id": "5cYxPAmwyAXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### View and GlobalTempview and Tempview\n",
        "1. view is also persistent object same like table, but not store data, while executing view, its fetch data from table only.\n",
        "\n",
        "2. tempview is available only to a particular session. Its a dataframe object not sql object.\n",
        "\n",
        "3. global tempview available to all sessions and to access we need to use global_temp db. Its a dataframe object not sql object."
      ],
      "metadata": {
        "id": "qBg4sqGFHDWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fire_df.createGlobalTempView(\"fire_service_calls_view\")\n",
        "\n",
        "select * from global_temp.fire_service_calls_view"
      ],
      "metadata": {
        "id": "M1jcbw6YKVto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Managed vs External Tables\n",
        "1. saveAsTable abd Create table DDl by command both create Managed Table default\n",
        "2. **for Managed Table**, spark creates table at a pre-defined warehouse location. You can specify the location befor creating the cluster by specifying the \"spark.sql.warehouse.dir\" config. But you can't change once defined.\n",
        "3. Spark manages table metadata and table data together.\n",
        "4. spark deletes the table data and metadata both when you drop the table.\n",
        "\n",
        "\n",
        "1. **External Table** is a mechanism to share the data across the projects or different storage layers.\n",
        "2. using location in create table ddl command to create external table.\n",
        "3. You can't drop the external table (drop only metadata not the actual data), but can read, update or overwrite it like managed table."
      ],
      "metadata": {
        "id": "qY835OOyNHj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Managed Table\n",
        "%fs rm -r /user/hive/warehouse/flight_time_tbl\n",
        "\n",
        "flight_time_df.write \\\n",
        "            .format(\"parquet\") \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .saveAsTable(\"flight_time_tbl\")"
      ],
      "metadata": {
        "id": "E_XcCCQeO4SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "\n",
        "# External Table\n",
        "create table ext_flight_time_tbl like flight_time_tbl\n",
        "location \"/user/hive/warehouse/flight_time_tbl\""
      ],
      "metadata": {
        "id": "51v7rjseNPe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To get the table/view metadata"
      ],
      "metadata": {
        "id": "ox71k7JqIvR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "describe extended <table_name or view_name>\n",
        "\n",
        "show tables\n",
        "\n",
        "show views\n",
        "\n",
        "spark.catalog.listTables()"
      ],
      "metadata": {
        "id": "kCM0DlFQyAVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Work with UnStructered data in Spark\n",
        "1. load data -> discover schema -> apply column transformation \n",
        "\n",
        "2. Spark gives us **regexp_extract()** function to extract one field from a regular expression, and I have the code for that shown below. So I am using the select() method to take out four fields.The regexp_extract() method takes three arguments. The first one is the column on which you want to apply the regular expression. The second argument is the regular expression itself.The last argument is the position after splitting the value using the regular expression.\n"
      ],
      "metadata": {
        "id": "pA_YMcAmUPg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_extract\n",
        "\n",
        "log_reg = r''\n",
        "\n",
        "fire_df.select(regexp_extract(\"value\", log_reg, 1).alias(\"ip\"),\n",
        "               regexp_extract(\"value\", log_reg, 4).alias(\"date\"))"
      ],
      "metadata": {
        "id": "_-xPlLrCLMwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### count is action or transformation?\n",
        "1. DataFrame.count() -> Action\n",
        "2. GroupedData.count() -> Transformation"
      ],
      "metadata": {
        "id": "BwS2uC7VfbY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fire_df.select(\"CallType\")  \\\n",
        "    .where(\"CallType is not null\")  \\\n",
        "    .groupBy(\"CallType\") \\\n",
        "    .count() \\\n",
        "    .orderBy(\"count\", ascending=False) \\\n",
        "    .show(10)  # showing top 10 records only"
      ],
      "metadata": {
        "id": "F_pWyUbgflex"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}