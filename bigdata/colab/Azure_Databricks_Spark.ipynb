{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Azure_Databricks_Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPyqcRsYo7QowEUuX9YB4Nn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buaindra/gcp_utility/blob/main/bigdata/colab/Azure_Databricks_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Azure Databricks and Spark\n",
        "### Ref:\n",
        "1. Coursera Course: https://www.coursera.org/learn/perform-data-science-with-azure-databricks/lecture/Wn6zD/explain-azure-databricks\n",
        "2. Coursera Course: https://www.coursera.org/learn/big-data-integration-processing/home/week/1"
      ],
      "metadata": {
        "id": "erPC0ptSFVfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Engineering evolution\n",
        "1. Types of Data:\n",
        "  1. Sturctured Data (RDMS: MSSQL, Oracle, Postgres, MySQL)\n",
        "  2. Semi-Structured Data (Json, XML)\n",
        "  3. Un-Structured Data (files, Images, Videos)\n",
        "\n",
        "2. Bigdata Criteria:\n",
        "  1. Scalability\n",
        "    1. **Vertical scalling**: singale machine got higher CPU, Memory, Disk Storage and its vertically increase the single system. Its complex and time taking and old fashioned. \n",
        "    2. **Horizontal scalling**: multiple same machines added into the network. For distributed approch, its so faster to scale up and down horizontally.\n",
        "  2. **Fault-tolerance** and **High-availibility (HA)**\n",
        "  3. **Cost-Effectiveness**\n",
        "\n",
        "3. Bigdata solution approach:\n",
        "  1. Monolithic: One Standalone, Large and Robust system, to handle bigdata (Teradata, Exadata). Supports structured and semi-structured.\n",
        "  2. Distributed: Takes many smaller clusters to store data distributed way and process the data parallel manner.\n"
      ],
      "metadata": {
        "id": "aazBBRPnO2-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hadoop\n",
        "**Apache Hadoop** is an open source framework that is used to efficiently store and process large datasets ranging in size from gigabytes to petabytes of data. Instead of using one large computer to store and process the data, Hadoop allows clustering multiple computers to analyze massive datasets in parallel more quickly.\n",
        "\n",
        "> Distributed computing and parallel processing\n",
        "\n"
      ],
      "metadata": {
        "id": "-yAnQ8-hABzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hadoop consists of four main core modules:\n",
        "1. Hadoop Distributed File System (HDFS) –\n",
        "  > A distributed file system that runs on standard or low-end hardware. HDFS provides better data throughput than traditional file systems, in addition to high fault tolerance and native support of large datasets.\n",
        "\n",
        "  > Component:\n",
        "    1. Name Node (NN):\n",
        "      1. NN keeping the records of file metadata (file name, size, directory location, file blocks, block id, block location etc.)\n",
        "    2. Data Node (DN):\n",
        "      1. Typically, in **Hadoop 2.0 Block size is 128 MB**.\n",
        "\n",
        "2. Yet Another Resource Negotiator (YARN) –\n",
        "  > Cluster Resourse Manager\n",
        "\n",
        "  > Manages and monitors cluster nodes and resource usage. It schedules jobs and tasks.\n",
        "\n",
        "  > Main Component:\n",
        "    1. RM: Resourse Manager\n",
        "    2. NM: Node Manager\n",
        "    3. AM: Application Master (*Need to check more*)\n",
        "      1. RM tells any one of the NM to create AM container, inside AM container single and individual application is being executing.\n",
        "       \n",
        "\n",
        "3. MapReduce – \n",
        "  > A framework that helps programs do the parallel computation on data. The map task takes input data and converts it into a dataset that can be computed in key value pairs. The output of the map task is consumed by reduce tasks to aggregate output and provide the desired result.\n",
        "\n",
        "  > Consepts:\n",
        "    1. Programming Model \n",
        "      1. Way of programing concepts to solve the problem.\n",
        "    2. Programming Framework\n",
        "      1. Consists of set of api, services.\n",
        "\n",
        "4. Hadoop Common – \n",
        "  > Provides common Java libraries that can be used across all modules.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QkIib8dNKPZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### On top of this Hadoop Core Systems there are other hadoop services:\n",
        "1. Hive: Relational database and accept simple sql query inside in it.\n",
        "2. HBase: NoSql database to perform large volumn dataset.\n",
        "3. Scoop: Ingestion Tool for import/export data to/from HDFS. (*Need to check more*)\n",
        "4. Oozie: mostly ETL tool. (*Need to check more*)\n",
        "5. Pig: Language similar like python. (*Need to check more*)\n",
        "6. Scala: Language similar like unix/python. (*Need to check more*)\n",
        "7. Fortron: Databricks SQL Analytics\n"
      ],
      "metadata": {
        "id": "LJd3lqlO_79-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scenarios in Hadoop development:\n",
        "\n",
        "1. Application Master Container:\n",
        "  1. One data processing program (Hadoop M/R program) = One AM Container + Many other mapper and reducers. It follows master/slave architecture. So AM container is the master and the other mapper/reducers are the slaves. Always remember, you can have only one master. So only one AM container for one application.\n",
        "\n",
        "2. If Map lost or failed:\n",
        "  1. Lost mapper\n",
        "    1. The mapper sends a heartbeat to the AM. If the AM doesn't receive a heartbeat, we assume the task failed or lost and the AM will start a new mapper for doing the work. \n",
        "\n",
        "  2. Failed mapper\n",
        "    1. If the mapper failed with some runtime exception, the JVM notifies the error to the AM and the AM marks the mapper as failed. We also have some configurations to retry failed mappers. So AM might retry the mapper depending on the configuration. But when all the retries failed, the application fails with an error/exception."
      ],
      "metadata": {
        "id": "HD5qNYATkXFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Lake and Data Warehouse basic concept:\n",
        "1. Data Lake\n",
        "  1. is a blob storage, where you can store structured, non-structured, semi-structured , any kind of data, files.\n",
        "  2. example: HDFS, S3, GCS, Azure Blob Storage, Cassendra File System.\n",
        "\n",
        "2. Data Warehouse \n",
        "  1. is a OLAP database, supports only structured data.\n",
        "  2. example: teradata, exadata.\n"
      ],
      "metadata": {
        "id": "FWeqwc2bmPQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Spark?\n",
        "Spark is data processing framework which workes on top of the distributed file system, cluster nodes and Resource Managers (Yarn, Mesos, Kubernates)\n",
        "\n",
        "1. Spark is a data processing platform.\n",
        "2. There are 2 ways to process the data in Spark:\n",
        "  1. **spark table/database and sql**\n",
        "    1. physical data storage layer: actual data store in backend file\n",
        "        1. normal db has .dba file, only support structured data\n",
        "        2. spark support avro, csv, parquet file, any types of file, unstructured or semi structured data and distributed storage (s3, hdfs, ADLS) as well.\n",
        "\n",
        "    2. logical data layer: have row and column display of data\n",
        "\n",
        "    3. metadata layer: consists of table schema, table creation date \n",
        "and other info.\n",
        "\n",
        "  2. **dataframe**\n",
        "    1. spark df is similar like spark table without metadata store.\n",
        "    2. it has runtime metadata store which is called **catalog**.\n",
        "    3. spark df is runtime memory object  and its also got deleted once application finished."
      ],
      "metadata": {
        "id": "B7EiGI40uqRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Key Notes:\n",
        "1. Spark is **100x times faster** than Map-Reduce. Its actually do parallel jobs on distributed data.\n",
        "2. Spark can be easily worked with Python, Scala, Java, R, **ease of use**.\n",
        "3. Sparks **combines SQL, streaming, complex analytics** (Machine Learning)\n",
        "4. Spark **can run anywhare** (Apache Hadoop, Apache Mesos, Databricks, Kubernates, Standalone Cluster etc.)\n",
        "5. *spark dataframe and pandas dataframes* are not similar. **Dataframe** is a data structure and inside it we can perform various operations.\n",
        "6. Spark Solution:\n",
        "  1. Spark with Hadoop: Datalake\n",
        "  2. Spark without hadoop: lakehouse\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RwJ8-jjoKk09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spark RDD vs Dataframes \n",
        "1. Spark recommend to use Dataframe now. Spark Dataframe is a lazy evaluation on top of RDD."
      ],
      "metadata": {
        "id": "TIHfVHgZw6j6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Databricks\n",
        "1. Databricks is a ETL tools. It supports spark.\n",
        "2. Databricks uses optimized spark on top of apache spark which is 5 times faster than apache spark.\n",
        "3. Databricks only available on cloud (Azure, GCP, AWS).\n"
      ],
      "metadata": {
        "id": "KWKyDthzGJWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Databricks Community Login\n",
        "1. Ref: https://community.cloud.databricks.com/login.html\n",
        "\n",
        "#### Databricks Workspace menu:\n",
        "1. Workspace Type:\n",
        "  1. Data Science & Engineering:\n",
        "  2. Machine Learning: \n",
        "\n",
        "2. Compute\n",
        "  1. All-purpose clusters: suitable for dev and test work.\n",
        "  2. Job cluster: not available for community edition and its similar with ephemeral gcp cluster (when job finished, cluster will be shut down).\n",
        "\n",
        "3. Jobs:\n",
        "  1. this option is also not available from community edition\n",
        "\n",
        "4. Settings:\n",
        "  1. user settings:\n",
        "    1. you can add git links, change passwords.\n",
        "  2. Admin Console:\n",
        "    1. you can add users.\n",
        "    2. you can provide global init (shell script) to install anything on cluster before spark run and which are not installed via databricks runtime.\n",
        "\n",
        "5. Create:\n",
        "  1. Notebook\n",
        "  2. table\n",
        "  3. cluster"
      ],
      "metadata": {
        "id": "bcjcXeUNPwLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delta Lake\n",
        "  1. Its provide ACID transaction and data consistency feature.\n",
        "  2. **How to improve data processing in databricks**:\n",
        "    1. blogs from Databricks: https://databricks.com/blog/2022/05/20/five-simple-steps-for-implementing-a-star-schema-in-databricks-with-delta-lake.html"
      ],
      "metadata": {
        "id": "dKVBLPmB9x5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PySpark Ref:\n",
        "1. Youtube: \n",
        "  1. https://www.youtube.com/watch?v=_C8kWso4ne4&t=597s\n",
        "2. Spark Official Doc:\n",
        "  1. Python API Ref: https://spark.apache.org/docs/latest/api/python/reference/index.html\n"
      ],
      "metadata": {
        "id": "upru3DSMGMUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PySpark:\n",
        "1. *PySpark is an interface for Apache Spark in Python*, is often used for large scale data.\n",
        "2. create and start **SparkSession** before writing pyspark\n",
        "3. when using **inferSchema=True** while read data from csv, please check the file size, as if inferSchema enabled, its reads the whole file once and provide the datatype of columns based on the data, otherwise, *by default all columns will be string*.\n",
        "4. Check columns datatype or statistics on spark dataframe\n",
        "  1. **printSchema(), dtypes, describe()**\n",
        "5. check: **show()** and **collect()**\n",
        "6. check: **select()** and **withcolumns()**\n",
        "7. check: **na.drop**, **na.fill**\n",
        "7. check: **display**"
      ],
      "metadata": {
        "id": "Seb-_dRoAQRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install PySpark"
      ],
      "metadata": {
        "id": "rAcuQOdkUVvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark\n",
        "\n",
        "# Successfully installed py4j-0.10.9.3 pyspark-3.2.1"
      ],
      "metadata": {
        "id": "vBPxt0tzSNlx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb91b7e7-e089-445d-c376-604325814cda"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/py4j/\u001b[0m\n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 52.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=9d9d326ea65f899261b843ca8674d2f9be89ccafc572ca2fee4306a5a547d24d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Understand Pandas Dataframe"
      ],
      "metadata": {
        "id": "ZzZgDugkY84o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/sample_emp.csv\")\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU_M6u1nW_S1",
        "outputId": "b0abe649-547f-44d4-b01a-2c28d0471633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Name   age  Experience   Salary\n",
            "0      Krish  31.0        10.0  30000.0\n",
            "1  Sudhanshu  30.0         8.0  25000.0\n",
            "2      Sunny  29.0         4.0  20000.0\n",
            "3       Paul  24.0         3.0  20000.0\n",
            "4     Harsha  21.0         1.0  15000.0\n",
            "5    Shubham  23.0         2.0  18000.0\n",
            "6     Mahesh   NaN         NaN  40000.0\n",
            "7        NaN  34.0        10.0  38000.0\n",
            "8        NaN  36.0         NaN      NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to start writing PySpark"
      ],
      "metadata": {
        "id": "CaSSOD4cZB5H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BUZ7Vl9GFOEt"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#spark = SparkSession.builder.appName(\"Practise\").getOrCreate()\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SparkSession"
      ],
      "metadata": {
        "id": "ywQ7qNIgGQQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# debug SparkSession variable\n",
        "spark\n",
        "# print(spark.getActiveSession)\n",
        "# print(spark.version)\n",
        "# print(spark.conf)\n",
        "# print(spark.sparkContext)\n",
        "# print(spark._instantiatedSession)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "8ZSCGhB-GMdo",
        "outputId": "b057c8e9-a6be-4c22-d037-fe85ec2a948f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fc5d540a4d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4aa31c1a7845:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to read csv file in pyspark"
      ],
      "metadata": {
        "id": "6e8hwoB_FCWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diamonds_df = spark.read.format(\"csv\") \\\n",
        "                .option(\"header\", \"true\") \\\n",
        "                .option(\"inferSchema\", \"true\") \\\n",
        "                .load(\"/content/diamonds.csv\")\n",
        "\n",
        "diamonds_df.show(10)"
      ],
      "metadata": {
        "id": "WweUxatsukMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ps = spark.read.csv(\"/content/sample_emp.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "XwEeOtDXYvnO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fetch the data from spark dataframs"
      ],
      "metadata": {
        "id": "45ol6UWhGB0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display all dataframe rows and columns\n",
        "print(df_ps.show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg-6FhwMFpmt",
        "outputId": "bd0303f8-9c98-4575-f552-1d2c05e05591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+----------+------+\n",
            "|     Name| age|Experience|Salary|\n",
            "+---------+----+----------+------+\n",
            "|    Krish|  31|        10| 30000|\n",
            "|Sudhanshu|  30|         8| 25000|\n",
            "|    Sunny|  29|         4| 20000|\n",
            "|     Paul|  24|         3| 20000|\n",
            "|   Harsha|  21|         1| 15000|\n",
            "|  Shubham|  23|         2| 18000|\n",
            "|   Mahesh|null|      null| 40000|\n",
            "|     null|  34|        10| 38000|\n",
            "|     null|  36|      null|  null|\n",
            "+---------+----+----------+------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display 2 records from top\n",
        "print(df_ps.show(2))  # similar with head(2) as its also showing top 2 records \n",
        "print(df_ps.head(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWduuTqbKbwi",
        "outputId": "54c4816c-9a55-4a02-e48f-95ee9f2fc8fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+----------+------+\n",
            "|     Name|age|Experience|Salary|\n",
            "+---------+---+----------+------+\n",
            "|    Krish| 31|        10| 30000|\n",
            "|Sudhanshu| 30|         8| 25000|\n",
            "+---------+---+----------+------+\n",
            "only showing top 2 rows\n",
            "\n",
            "None\n",
            "[Row(Name='Krish', age=31, Experience=10, Salary=30000), Row(Name='Sudhanshu', age=30, Experience=8, Salary=25000)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display 2 records from down\n",
        "print(df_ps.tail(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVFJCmccKhYc",
        "outputId": "0a32cca6-f05a-4752-8732-0f38c86dbbf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(Name=None, age=34, Experience=10, Salary=38000), Row(Name=None, age=36, Experience=None, Salary=None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check datatypes of dataframe\n",
        "1. using **printSchema()** method\n",
        "2. or using **dtypes** property\n"
      ],
      "metadata": {
        "id": "1IymH9nqY4sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display the dataframe schema\n",
        "# in pandas its, df.info()\n",
        "print(df_ps.printSchema())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlVZk3fGLNcn",
        "outputId": "5b2d117a-8b19-4397-e758-23f999bf073e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- Experience: integer (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ps.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwLHm0LGZA3t",
        "outputId": "fdc48e65-d05e-485f-bc5e-bf497baf591d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Name', 'string'), ('age', 'int'), ('Experience', 'int'), ('Salary', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Computes basic statistics for numeric and string columns.\n",
        "1. using **describe()**"
      ],
      "metadata": {
        "id": "oRsLbMfVffyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_ps.describe())\n",
        "\n",
        "print(df_ps.describe().show())\n",
        "\n",
        "print(df_ps.describe(\"age\").show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qg_u3xsd5GY",
        "outputId": "6286f35b-363a-4924-8b16-5321c7c8bec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[summary: string, Name: string, age: string, Experience: string, Salary: string]\n",
            "+-------+------+------------------+------------------+-----------------+\n",
            "|summary|  Name|               age|        Experience|           Salary|\n",
            "+-------+------+------------------+------------------+-----------------+\n",
            "|  count|     7|                 8|                 7|                8|\n",
            "|   mean|  null|              28.5| 5.428571428571429|          25750.0|\n",
            "| stddev|  null|5.3718844791323335|3.8234863173611093|9361.776388210581|\n",
            "|    min|Harsha|                21|                 1|            15000|\n",
            "|    max| Sunny|                36|                10|            40000|\n",
            "+-------+------+------------------+------------------+-----------------+\n",
            "\n",
            "None\n",
            "+-------+------------------+\n",
            "|summary|               age|\n",
            "+-------+------------------+\n",
            "|  count|                 8|\n",
            "|   mean|              28.5|\n",
            "| stddev|5.3718844791323335|\n",
            "|    min|                21|\n",
            "|    max|                36|\n",
            "+-------+------------------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display the dataframe columns\n",
        "print(df_ps.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NSiXKsQLNbI",
        "outputId": "1d009c09-3376-490b-a563-65692b1bc720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Name', 'age', 'Experience', 'Salary']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Differebce between show() and collect()\n",
        "\n",
        "\n",
        "|id | show() | collect()|\n",
        "|--- | --- | ---|\n",
        "|1 | Returns None, display the rows and columns as tabular format | Returns  all the records as list|\n",
        "\n",
        "\n",
        "```python\n",
        "# similar \n",
        "df_ps.select([\"Name\", \"age\"]).show()\n",
        "df_ps.select(\"Name\", \"age\").show()\n",
        "\n",
        "df_ps.select([\"Name\", \"age\"]).collect()\n",
        "df_ps.select(\"Name\", \"age\").collect()\n",
        "```"
      ],
      "metadata": {
        "id": "b6AvEtq2RBOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display specific columns and custom columns from dataframes\n",
        "\n",
        "df_ps_retirement_yr = df_ps.select(\"Name\", \"age\", (60-df_ps.age).alias(\"retirement_yr_remaining\")).collect() # create list of rows\n",
        "print(df_ps_retirement_yr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_97kx6kLNU0",
        "outputId": "d2176663-09c7-43a3-8b88-ef9c50f013d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(Name='Krish', age=31, retirement_yr_remaining=29), Row(Name='Sudhanshu', age=30, retirement_yr_remaining=30), Row(Name='Sunny', age=29, retirement_yr_remaining=31), Row(Name='Paul', age=24, retirement_yr_remaining=36), Row(Name='Harsha', age=21, retirement_yr_remaining=39), Row(Name='Shubham', age=23, retirement_yr_remaining=37), Row(Name='Mahesh', age=None, retirement_yr_remaining=None), Row(Name=None, age=34, retirement_yr_remaining=26), Row(Name=None, age=36, retirement_yr_remaining=24)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to add new/custom columns into dataframe using **select()** and **withcolumns()**\n",
        "#### Differences betwen select() and withcolumns()\n",
        "\n",
        "| id | select() | withcolumns() |\n",
        "|---|---|---|\n",
        "|1| select is used for to select specific columns | withColumns is used to add a new custom column|\n"
      ],
      "metadata": {
        "id": "1ZT4PSlhhPUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_select = df_ps.select(\"name\",  \"age\", (60-df_ps.age).alias(\"retirement_yr_remaining\"))\n",
        "print(df_out_select.show())\n",
        "\n",
        "df_out_withcolumn = df_ps.withColumn(\"retirement_yr_remaining\", 60-df_ps.age)\n",
        "# df_out_withcolumn = df_ps.withColumn(\"retirement_yr_remaining\", 60-df_ps[\"age\"])  # df_ps.age and df_ps[\"age\"] are same\n",
        "print(df_out_withcolumn.show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3lWoAH4Sm7m",
        "outputId": "87b739a7-d597-446e-c456-dacd7025e812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+-----------------------+\n",
            "|     name| age|retirement_yr_remaining|\n",
            "+---------+----+-----------------------+\n",
            "|    Krish|  31|                     29|\n",
            "|Sudhanshu|  30|                     30|\n",
            "|    Sunny|  29|                     31|\n",
            "|     Paul|  24|                     36|\n",
            "|   Harsha|  21|                     39|\n",
            "|  Shubham|  23|                     37|\n",
            "|   Mahesh|null|                   null|\n",
            "|     null|  34|                     26|\n",
            "|     null|  36|                     24|\n",
            "+---------+----+-----------------------+\n",
            "\n",
            "None\n",
            "+---------+----+----------+------+-----------------------+\n",
            "|     Name| age|Experience|Salary|retirement_yr_remaining|\n",
            "+---------+----+----------+------+-----------------------+\n",
            "|    Krish|  31|        10| 30000|                     29|\n",
            "|Sudhanshu|  30|         8| 25000|                     30|\n",
            "|    Sunny|  29|         4| 20000|                     31|\n",
            "|     Paul|  24|         3| 20000|                     36|\n",
            "|   Harsha|  21|         1| 15000|                     39|\n",
            "|  Shubham|  23|         2| 18000|                     37|\n",
            "|   Mahesh|null|      null| 40000|                   null|\n",
            "|     null|  34|        10| 38000|                     26|\n",
            "|     null|  36|      null|  null|                     24|\n",
            "+---------+----+----------+------+-----------------------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to drop columns from pyspark dataframes"
      ],
      "metadata": {
        "id": "F8p_xWEHoad1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_out = df_out_withcolumn.drop(\"retirement_yr_remaining\")\n",
        "print(\"After dropping column\", df_out.show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTUxh6UNLNOj",
        "outputId": "48268eb8-1186-441a-cd5b-02eaaae58a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+----------+------+\n",
            "|     Name| age|Experience|Salary|\n",
            "+---------+----+----------+------+\n",
            "|    Krish|  31|        10| 30000|\n",
            "|Sudhanshu|  30|         8| 25000|\n",
            "|    Sunny|  29|         4| 20000|\n",
            "|     Paul|  24|         3| 20000|\n",
            "|   Harsha|  21|         1| 15000|\n",
            "|  Shubham|  23|         2| 18000|\n",
            "|   Mahesh|null|      null| 40000|\n",
            "|     null|  34|        10| 38000|\n",
            "|     null|  36|      null|  null|\n",
            "+---------+----+----------+------+\n",
            "\n",
            "After dropping column None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to rename a column into Dataframe"
      ],
      "metadata": {
        "id": "wejKFty-ph_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_renamed = df_out.withColumnRenamed(\"Age\",\"Actual Age\")\n",
        "print(df_out.show())  # no changes reflected on original dataframe\n",
        "print(df_out_renamed.show())  # changes reflected on returned dataframe after withColumnRenamed()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRH_WoLwLNJB",
        "outputId": "d32aee4a-fe3f-4897-d5ad-a4973f7a36c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+----------+------+\n",
            "|     Name| age|Experience|Salary|\n",
            "+---------+----+----------+------+\n",
            "|    Krish|  31|        10| 30000|\n",
            "|Sudhanshu|  30|         8| 25000|\n",
            "|    Sunny|  29|         4| 20000|\n",
            "|     Paul|  24|         3| 20000|\n",
            "|   Harsha|  21|         1| 15000|\n",
            "|  Shubham|  23|         2| 18000|\n",
            "|   Mahesh|null|      null| 40000|\n",
            "|     null|  34|        10| 38000|\n",
            "|     null|  36|      null|  null|\n",
            "+---------+----+----------+------+\n",
            "\n",
            "None\n",
            "+---------+----------+----------+------+\n",
            "|     Name|Actual Age|Experience|Salary|\n",
            "+---------+----------+----------+------+\n",
            "|    Krish|        31|        10| 30000|\n",
            "|Sudhanshu|        30|         8| 25000|\n",
            "|    Sunny|        29|         4| 20000|\n",
            "|     Paul|        24|         3| 20000|\n",
            "|   Harsha|        21|         1| 15000|\n",
            "|  Shubham|        23|         2| 18000|\n",
            "|   Mahesh|      null|      null| 40000|\n",
            "|     null|        34|        10| 38000|\n",
            "|     null|        36|      null|  null|\n",
            "+---------+----------+----------+------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### how to drop rows which has null values\n",
        "\n",
        "##### drop parameters:\n",
        "1. *how* : str, optional\n",
        "  1. 'any' or 'all'.\n",
        "  2. If 'any', drop a row if it contains any nulls.\n",
        "  3. If 'all', drop a row only if all its values are null.\n",
        "\n",
        "2. *thresh*: int, optional\n",
        "  1. default None\n",
        "  2. If specified, drop rows that have less than thresh non-null values.\n",
        "  3. This overwrites the how parameter.\n",
        "\n",
        "3. *subset*: str, tuple or list, optional\n",
        "  1. optional list of column names to consider having null will be deleted."
      ],
      "metadata": {
        "id": "tkKQ3fyktbuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_out_renamed.na.drop(how=\"any\", thresh=1).show())\n",
        "print(df_out_renamed.na.drop(how=\"any\", subset=[\"Experience\"]).show())\n",
        "print(df_out_renamed.na.drop().show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyX3NydALM-Y",
        "outputId": "89d37ef0-c70c-44cd-fbd7-8295405e3c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+----------+------+\n",
            "|     Name|Actual Age|Experience|Salary|\n",
            "+---------+----------+----------+------+\n",
            "|    Krish|        31|        10| 30000|\n",
            "|Sudhanshu|        30|         8| 25000|\n",
            "|    Sunny|        29|         4| 20000|\n",
            "|     Paul|        24|         3| 20000|\n",
            "|   Harsha|        21|         1| 15000|\n",
            "|  Shubham|        23|         2| 18000|\n",
            "|   Mahesh|      null|      null| 40000|\n",
            "|     null|        34|        10| 38000|\n",
            "|     null|        36|      null|  null|\n",
            "+---------+----------+----------+------+\n",
            "\n",
            "None\n",
            "+---------+----------+----------+------+\n",
            "|     Name|Actual Age|Experience|Salary|\n",
            "+---------+----------+----------+------+\n",
            "|    Krish|        31|        10| 30000|\n",
            "|Sudhanshu|        30|         8| 25000|\n",
            "|    Sunny|        29|         4| 20000|\n",
            "|     Paul|        24|         3| 20000|\n",
            "|   Harsha|        21|         1| 15000|\n",
            "|  Shubham|        23|         2| 18000|\n",
            "|     null|        34|        10| 38000|\n",
            "+---------+----------+----------+------+\n",
            "\n",
            "None\n",
            "+---------+----------+----------+------+\n",
            "|     Name|Actual Age|Experience|Salary|\n",
            "+---------+----------+----------+------+\n",
            "|    Krish|        31|        10| 30000|\n",
            "|Sudhanshu|        30|         8| 25000|\n",
            "|    Sunny|        29|         4| 20000|\n",
            "|     Paul|        24|         3| 20000|\n",
            "|   Harsha|        21|         1| 15000|\n",
            "|  Shubham|        23|         2| 18000|\n",
            "+---------+----------+----------+------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to fill/handle missing values (null) in dataframe"
      ],
      "metadata": {
        "id": "NTDptUDxyA31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_out_renamed.na.fill(\"missing\", subset=[\"Name\", \"Actual Age\",\"Experience\"]).show())  # doesn't affect \"Actual Age\",\"Experience\" as their datatype is int\n",
        "print(df_out_renamed.na.fill({'name': 'missing', 'actual age': 0, 'salary': 0.00}).show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqFcqze2yAgn",
        "outputId": "34bf340c-c6bd-4aa7-f975-b704787a909b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+----------+------+\n",
            "|     Name|Actual Age|Experience|Salary|\n",
            "+---------+----------+----------+------+\n",
            "|    Krish|        31|        10| 30000|\n",
            "|Sudhanshu|        30|         8| 25000|\n",
            "|    Sunny|        29|         4| 20000|\n",
            "|     Paul|        24|         3| 20000|\n",
            "|   Harsha|        21|         1| 15000|\n",
            "|  Shubham|        23|         2| 18000|\n",
            "|   Mahesh|      null|      null| 40000|\n",
            "|  missing|        34|        10| 38000|\n",
            "|  missing|        36|      null|  null|\n",
            "+---------+----------+----------+------+\n",
            "\n",
            "None\n",
            "+---------+----------+----------+------+\n",
            "|     Name|Actual Age|Experience|Salary|\n",
            "+---------+----------+----------+------+\n",
            "|    Krish|        31|        10| 30000|\n",
            "|Sudhanshu|        30|         8| 25000|\n",
            "|    Sunny|        29|         4| 20000|\n",
            "|     Paul|        24|         3| 20000|\n",
            "|   Harsha|        21|         1| 15000|\n",
            "|  Shubham|        23|         2| 18000|\n",
            "|   Mahesh|         0|      null| 40000|\n",
            "|  missing|        34|        10| 38000|\n",
            "|  missing|        36|      null|     0|\n",
            "+---------+----------+----------+------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imputer (\\** *Need to learn more about* )\n",
        "1. Imputer also helps in handling null values in spark dataframe by Mean, Median values."
      ],
      "metadata": {
        "id": "FYEunEA_ARM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "imputer = Imputer(\n",
        "    inputCols=['age', 'Experience', 'Salary'], \n",
        "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
        "    ).setStrategy(\"median\")\n",
        "\n",
        "# Add imputation cols to df\n",
        "imputer.fit(df_ps).transform(df_ps).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zhk4Ac8yAfD",
        "outputId": "6cccd788-6c1b-4f3c-a4ea-ea091fdbf0c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+----------+------+-----------+------------------+--------------+\n",
            "|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
            "+---------+----+----------+------+-----------+------------------+--------------+\n",
            "|    Krish|  31|        10| 30000|         31|                10|         30000|\n",
            "|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n",
            "|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
            "|     Paul|  24|         3| 20000|         24|                 3|         20000|\n",
            "|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
            "|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
            "|   Mahesh|null|      null| 40000|         29|                 4|         40000|\n",
            "|     null|  34|        10| 38000|         34|                10|         38000|\n",
            "|     null|  36|      null|  null|         36|                 4|         20000|\n",
            "+---------+----+----------+------+-----------+------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to filter the spark dataframe\n",
        "1. if you want to add multiple filter, separate the each filter using ()\n",
        "2. use \"~\" as not, \"|\" as or , \"&\" as and"
      ],
      "metadata": {
        "id": "JJVOddU8TSLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df_ps.filter(\"age > 21\").show())\n",
        "# print(df_ps.filter(df_ps.age > 21, ).show())\n",
        "print(df_ps.filter(df_ps[\"age\"] > 21).show())  # recommend to use, others also same in nature\n",
        "# print(df_ps.where(\"age > 21\").show())\n",
        "# print(df_ps.where(df_ps.age > 21, ).show())\n",
        "# print(df_ps.where(df_ps[\"age\"] > 21).show())"
      ],
      "metadata": {
        "id": "EVFKGnsEyAbx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61ce3bf6-bb3a-4ad8-a4c9-bf5028278ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+----------+------+\n",
            "|     Name|age|Experience|Salary|\n",
            "+---------+---+----------+------+\n",
            "|    Krish| 31|        10| 30000|\n",
            "|Sudhanshu| 30|         8| 25000|\n",
            "|    Sunny| 29|         4| 20000|\n",
            "|     Paul| 24|         3| 20000|\n",
            "|  Shubham| 23|         2| 18000|\n",
            "|     null| 34|        10| 38000|\n",
            "|     null| 36|      null|  null|\n",
            "+---------+---+----------+------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### use where and selcet same time on spark Dataframe similar like sql "
      ],
      "metadata": {
        "id": "5RZrykLUoiqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ps.where((df_ps[\"Salary\"] >= 20000) & (df_ps[\"Salary\"] < 35000)).select(df_ps[\"Name\"], df_ps[\"Age\"], df_ps[\"Salary\"]).show()\n",
        "# df_ps.where(df_ps[\"Salary\"] >= 20000).where(df_ps[\"Salary\"] < 35000).select(df_ps[\"Name\"], df_ps[\"Age\"], df_ps[\"Salary\"]).show()"
      ],
      "metadata": {
        "id": "RGn0FeaCyAZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cee6fb9-6f54-4d70-dd62-589128926dd2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+------+\n",
            "|     Name|Age|Salary|\n",
            "+---------+---+------+\n",
            "|    Krish| 31| 30000|\n",
            "|Sudhanshu| 30| 25000|\n",
            "|    Sunny| 29| 20000|\n",
            "|     Paul| 24| 20000|\n",
            "+---------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### get the data from the spark dataframe using groupBy"
      ],
      "metadata": {
        "id": "xbcJgAzHuOVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "results_df = diamonds_df.select(\"color\", \"price\") \\\n",
        "              .groupBy(\"color\") \\\n",
        "              .agg(avg(\"price\")) \\\n",
        "              .sort(\"color\")\n",
        "\n",
        "results_df.show()"
      ],
      "metadata": {
        "id": "O53prD0cuTLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### link a temp view with this spark dataframe for sql query"
      ],
      "metadata": {
        "id": "F_vr9mhNkDOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diamonds_df.createOrReplaceTempView(\"tbl_diamonds\")\n",
        "diamonds_df = spark.sql(\"select * from tbl_diamonds where price <= 350\")\n",
        "diamonds_df.show()\n",
        "spark.catalog.dropTempView(\"tbl_diamonds\")"
      ],
      "metadata": {
        "id": "5cYxPAmwyAXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kCM0DlFQyAVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_-xPlLrCLMwE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}