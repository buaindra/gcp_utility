{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_Interview_Notes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6ewZDQSXaLdCqlSrK/2PR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buaindra/gcp_utility/blob/main/bigdata/Spark_Interview_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHox7Re8FcYP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Interview Notes"
      ],
      "metadata": {
        "id": "hpcYApoTFg6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Evaluation\n",
        "**Ref: Prashant Sir's Scholernest session, week 11**"
      ],
      "metadata": {
        "id": "VmbTZQ0dFkyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Map, reduce stores** intermediate results into disk and there is no further functions available to write the map code. \n",
        "Spark helps to store the intermediate results into memory and make available lots of functions for the spark developer.\n",
        "Spark also introduced RDD data structure like list in python. So java, python, scala multiple languages support spark programming.\n",
        "\n",
        "2. **Catalyst optimizer** optimize the SQL, daraframe, dataset already, no need to use rdd. And spark also doesn't recommend to use rdd directly.\n",
        "\n",
        "3. **Catalyst optimizer** comes on top of the rdd, and it creates optimized rdd from SQL and daraframe. So if you directly access rdd api then it will skip the optimization.\n",
        "\n",
        "4. Sample SQL and Dataframe:\n",
        "> Spark SQL: select * from table\n",
        ">\n",
        "> Spark daraframe: df.select(\"*\")\n",
        ">\n"
      ],
      "metadata": {
        "id": "fm34Gc30GOMN"
      }
    }
  ]
}