{"cells":[{"cell_type":"markdown","source":["#### Create SparkSession"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4ed1323-366e-4ea2-9d90-6d2dc7a488b7"}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n            .appName(\"test_dataframe\") \\\n            .getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39e74512-caf3-4ece-aebd-50fd3b3949af"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### how to create spark dataframe:\n  1. read\n  ```python\n    df1 = spark.read \\\n            .format(\"csv\") \\\n            .option(\"header\", \"true\") \\\n            .option(\"inferSchema\", \"true\") \\\n            .load(\"sample.txt)\n  ```\n  2. sql()\n  ```python\n    df2 = spark.sql(\"\"\"\n    select count(distinct calltype) as distinct_call_type from fire_service_call_view where calltype is not NULL\n    \"\"\")\n  ```\n  3. table(): convert spark table to spark dataframe\n  ```python\n    df3 = spark.table(\"spark_db_name.table_name\")\n  ```\n  4. range(): create single column dataframe\n  ```python\n    df4 = spark.range(5)\n  ```\n  5. createDataFrame(): converts a list (python list, Spark Row, panda dataframe, RDD) into a spark dataframe\n  ```python\n    df1 = spark.createDataFrame(data_list)\n  ```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26361ff5-836c-4a3a-b719-47b33933e925"}}},{"cell_type":"code","source":["df1 = spark.range(5)\ndf1.printSchema()\ndf1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5869810-169d-4281-b988-eeb211b1ff39"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from datetime import datetime, date\ndata_list = [(1, 2., \"string1\", date(2022,1,1), datetime(2022,1,1,12,0)),\n            (2, 3., \"string2\", date(2022,2,1), datetime(2022,1,2,12,0)),\n            (3, 4., \"string3\", date(2022,3,1), datetime(2022,1,3,12,0))]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45154114-764b-4bb8-9e46-30b8683610ce"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df1 = spark.createDataFrame(data_list).toDF(\"a\",\"b\",\"c\",\"d\",\"e\")\ndf1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbd1d079-8ac2-4592-8aab-99534ddf9d19"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df1.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f442814d-3b8a-4dd9-8eef-65bb5b7e51b3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["schema_1 = [\"a\",\"b\",\"c\",\"d\",\"e\"] \nschema_2 = \"a int, b double, c string, d date, e timestamp\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2746015-4b56-4833-9399-95a6b8293a89"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.createDataFrame(data_list, schema=schema_1).printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6714544-6dbc-4f16-bda0-eb19aac914ad"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.createDataFrame(data_list, schema=schema_2).printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43bc123f-2bea-41c9-b78c-ac44de741398"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Row\n\nrow_list = [Row(a=1, b=2., c=\"string1\", d=date(2022,1,1), e=datetime(2022,1,1,12,0)),\n            Row(a=2, b=3., c=\"string2\", d=date(2022,2,1), e=datetime(2022,1,2,12,0)),\n            Row(a=3, b=4., c=\"string3\", d=date(2022,3,1), e=datetime(2022,1,3,12,0))]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25a1b50d-314f-4252-b871-4a15128e7c81"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.createDataFrame(row_list).printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ef14637-f74f-4c8a-a159-b83e1a32312b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\n\npd_df = pd.DataFrame({\"a\": [1,2,3],\n                     \"b\": [2.,3.,4.],\n                     \"c\": [\"string1\", \"string2\", \"string3\"],\n                     \"d\": [date(2022,1,1),date(2022,2,1),date(2022,3,1)],\n                     \"e\": [datetime(2022,1,1,12,0),datetime(2022,1,2,12,0),datetime(2022,1,3,12,0)]})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf69595e-92ae-4215-85f5-39778e63539d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.createDataFrame(pd_df, schema_2).printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93630a82-7abf-4804-8cc1-206e05bee45e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd = spark.sparkContext.parallelize([(1, 2., \"string1\", date(2022,1,1), datetime(2022,1,1,12,0)),\n            (2, 3., \"string2\", date(2022,2,1), datetime(2022,1,2,12,0)),\n            (3, 4., \"string3\", date(2022,3,1), datetime(2022,1,3,12,0))])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fcc7913c-0643-4a86-808e-d4de6a7b30fd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.createDataFrame(rdd, schema_1).printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0336f6c5-ff7c-4cb8-a390-ee8d0b135f27"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%fs ls /FileStore/tables\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9db711a-b377-4d1b-a3ce-f958d4f99489"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["flight_schema_ddl = \"\"\"FL_DATE date, OP_CARRIER STRING, OP_CARRIER_FL_NUM INT, ORIGIN STRING, \n             ORIGIN_CITY_NAME STRING, DEST STRING, DEST_CITY_NAME STRING, CRS_DEP_TIME INT, DEP_TIME INT, \n             WHEELS_ON INT, TAXI_IN INT, CRS_ARR_TIME INT, ARR_TIME INT, CANCELLED STRING, DISTANCE INT\"\"\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93780d76-39ff-4124-90a6-9770ecf43288"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["1. Schema Inference is not advisable\n1. often date, timestamp, boolean datatype can't store correct value into dataframe. Spark dataframe expect date in yyyy-mm-dd format normally. So if the data not loaded properly , it can store null without generating error. So add **mode** option.\n2. **mode**: FAILFAST, DROPMALFORMED, PERMISSIVE\n3. if there are 2 date columns, one is dd-mm-yyyy and another one is yyyy-mm-dd, then simply load the data as string datatype.\n4. enforce schema explicitely."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"881816fe-6c3f-4de4-90b3-a910d2791b32"}}},{"cell_type":"code","source":["flight_time_raw_df = spark.read \\\n                        .format(\"json\") \\\n                        .schema(flight_schema_ddl) \\\n                        .option(\"mode\", \"FAILFAST\") \\\n                        .option(\"dateFormat\", \"M/d/y\") \\\n                        .load(\"/FileStore/tables/flight_time.json\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"695879a7-6ae3-44ad-b11b-69f7fe1c2e3b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["flight_time_raw_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ed6fad3-a27c-4feb-8f3e-43411c3f298e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(flight_time_raw_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7471a81-718f-4948-9cbd-c5abd2d8762a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nflight_schema_struct = StructType([\n    StructField(\"FL_DATE\", DateType()),\n    StructField(\"OP_CARRIER\", StringType()),\n    StructField(\"OP_CARRIER_FL_NUM\", IntegerType()),\n    StructField(\"ORIGIN\", StringType()),\n    StructField(\"ORIGIN_CITY_NAME\", StringType()),\n    StructField(\"DEST\", StringType()),\n    StructField(\"DEST_CITY_NAME\", StringType()),\n    StructField(\"CRS_DEP_TIME\", StringType()),\n    StructField(\"DEP_TIME\", StringType()),\n    StructField(\"WHEELS_ON\", StringType()),\n    StructField(\"TAXI_IN\", StringType()),\n    StructField(\"CRS_ARR_TIME\", StringType()),\n    StructField(\"ARR_TIME\", StringType()),\n    StructField(\"CANCELLED\", StringType()),\n    StructField(\"DISTANCE\", StringType()),\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de8ce2c1-e4f1-4f41-ae96-f07e8ba1a06f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["flight_time_raw_df1 = spark.read \\\n                        .format(\"json\") \\\n                        .schema(flight_schema_struct) \\\n                        .option(\"mode\", \"FAILFAST\") \\\n                        .option(\"dateFormat\", \"M/d/y\") \\\n                        .load(\"/FileStore/tables/flight_time.json\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd282ce2-1ad9-48b8-b183-962d91317990"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(flight_time_raw_df1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb73aaa3-8f19-487b-b840-45b317c8543c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### There are 2 approaches to write the dataframe\n1. read -> transform -> create view using dataframe -> create spark table -> Load into the table\n2. read -> transform -> save as a spark table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6e7a1e5-8e23-4668-95ec-dcd2bb0736bf"}}},{"cell_type":"code","source":["from pyspark.sql.functions import to_date, expr\n\nflight_time_df = flight_time_raw_df \\\n                    .withColumn(\"FL_DATE\", to_date(\"FL_DATE\", \"M/d/y\")) \\\n                    .withColumn(\"CANCELLED\", expr(\"if(CANCELLED==1, true, false)\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"470fe75f-adf4-4258-8b2b-532fa4927943"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### convert spark dataframe into a spark table \n1. dataframe write mode can be\n    1. append\n    2. overwrite\n    3. error **check\n    4."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db141924-a893-4bd5-add1-3b0d5ba668dd"}}},{"cell_type":"code","source":["%fs rm -r /user/hive/warehouse/flight_time_tbl"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ae19ef5-e0c9-4356-9a1c-41ac703ab3da"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["flight_time_df.write \\\n            .format(\"parquet\") \\\n            .mode(\"overwrite\") \\\n            .saveAsTable(\"flight_time_tbl\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d06cd7b-7ec8-4deb-b0ca-d7725a338ad5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\ndescribe extended flight_time_tbl"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f525866-cb07-49f6-a0bb-93e72e1e6f81"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%fs ls /user/hive/warehouse/flight_time_tbl"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70ac97eb-cf6d-4955-8387-9b04474b2735"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### How to check table metadata"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb6a8c33-9d63-4d5a-93f7-e1ae0f908126"}}},{"cell_type":"code","source":["%sql\nshow tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6d7809f-d8d4-4b4c-abd6-170c18049bba"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.catalog.listTables()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7bfd5af4-4cbd-41d4-97d3-9a1ef17ac04b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Managed vs External Tables\n1. saveAsTable abd Create table DDl by command both create Managed Table default\n2. **for Managed Table**, spark creates table at a pre-defined warehouse location. You can specify the location befor creating the cluster by specifying the \"spark.sql.warehouse.dir\" config. But you can't change once defined.\n3. Spark manages table metadata and table data together.\n4. spark deletes the table data and metadata both when you drop the table.\n\n\n1. **External Table** is a mechanism to share the data across the projects or different storage layers.\n2. using location in create table ddl command to create external table.\n3. You can't drop the external table (drop only metadata not the actual data), but can read, update or overwrite it like managed table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e196a544-0e01-4e98-b90b-0b49067f4ffb"}}},{"cell_type":"code","source":["%sql\n\ncreate table ext_flight_time_tbl like flight_time_tbl\nlocation \"/user/hive/warehouse/flight_time_tbl\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26e84be2-708f-4beb-aef0-5472e8249184"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\ndescribe extended ext_flight_time_tbl"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d06e067-8e72-40be-9b79-2b25f58c849a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.catalog.listTables()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e81a6f0-1a71-464f-a60d-62e2f19c9df6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\ndrop table ext_flight_time_tbl"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abe92f6a-0250-4c64-a9f7-0b018ea59bfd"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"04_Create_Read_Write_Dataframe","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3824036445153912}},"nbformat":4,"nbformat_minor":0}
