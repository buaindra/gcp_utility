{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNy49wUwts7pScAt5kjNmfL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buaindra/gcp_utility/blob/main/machine_learning/Machine_Learning_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning "
      ],
      "metadata": {
        "id": "4Tm5vMmlTVAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Machine Leraning Courses:\n",
        "1. Upgrad: https://learn.upgrad.com/course/3436 (Artificial Intelligence in the Real World)\n",
        "2. Google Foundation Course: https://developers.google.com/machine-learning/crash-course/ml-intro\n"
      ],
      "metadata": {
        "id": "Z1bWyNCWXJmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Machine Learning, Ref:\n",
        "1. https://www.youtube.com/watch?v=ukzFI9rgwfU&t=1s"
      ],
      "metadata": {
        "id": "iPLO8XxzcQUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model, Ref:\n",
        "1. https://www.youtube.com/watch?v=yN7ypxC7838\n",
        "2. https://www.youtube.com/watch?v=38SUUaMX5Rg"
      ],
      "metadata": {
        "id": "TZWY4dNlb5pM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Machine Learning Models:\n",
        "\n",
        "1. **Supervised Learning:**\n",
        "\t- This is based on labeled data.\n",
        "\t- This is learning from initial labeled dataset which train the model and Predictions and actions are taken based on this initial dataset.\n",
        "\n",
        "\t1. **Regression:**\n",
        "\t\t- Regression works on infinity/continuous value.\n",
        "\t\t- Has relationship between dependent and independent variable\n",
        "\t\t1. **Linear Regression:**\n",
        "\t\t\t- Its predict numerical value like an insurance premium or home value\n",
        "\t\t2. **Decision Tree:** \n",
        "\t\t\t- its based on nodes, more node you have, the decision tree will be accurate.\n",
        "\t\t3. **Random Forest:**\n",
        "\t\t\t- Ensemble Learning Technique.\n",
        "\t\t\t- builds of over decision trees, involve creating multiple decision trees using bootstrap data sets of original data and randomly selected subset of variables at each steps of the decision tree.\n",
        "\t\t\t- relying on \"Majority Wins\" model.\n",
        "\t\t4. **Neural Netwrok:**\n",
        "\t\t\t- Its a popular and multi layer model like neurons in humain brains\n",
        "\t\t\t- Input Layer -> Hidden Layer -> nth Hidden Layer -> Output Layer\n",
        "\n",
        "\t2. **Classification:**\n",
        "\t\t- Classification works on 2 or n number of fixed values or discrete values\n",
        "\t\t1. **Logistic Regression:**\n",
        "\t\t\t1. **Binary Logistic Regression:**\n",
        "\t\t\t  - Which predict one or two classes (yes/no, cat or dog image)\n",
        "\t\t\t2. **Multiclass logistic regression:**\n",
        "\t\t\t\t- which predict more than two classes (buy/hold/sell)\n",
        "\t\t2. **Support Vector Machine:**\n",
        "\t\t\t- Supervised classification technique that carries an objective to find a hyperlane in n-dimensional space that can distinctly classify the data points.\n",
        "\t\t3. **Naive Bayes:**\n",
        "\t\t\t- Its a classifier, acts as a probabilistic machine learning model used for classification task. \n",
        "\t\t\t- crux of the classifier is based on Bayes Theorem.\n",
        "\t\t4. **Decision Tree, Random Forest, Neural Netwrok** also works same like regression, but only difference is that, outcome is discrete rather than continuous.\n",
        "    \n",
        "2. **Un-Supervised Learning:**\n",
        "\t- Used to draw inferences and find patterns from input data without references to the labeled outcome.\n",
        "\t- Re-modeling themselves based on the data pattern.\n",
        "\t1. **Clustering:**\n",
        "\t\t- involves grouping of data points, frequently used for customer segmentation, fraud detection and document classification.\n",
        "\t\t- below models have different methods to find the clusters and aim to achieve the same thing.\n",
        "\t\t1. **K-means:**\n",
        "\t\t2. **Hierarchical:**\n",
        "\t\t3. **Mean shift:**\n",
        "\t\t4. **Density-based:**\n",
        "\n",
        "\t2. **Dimensionality Reduction:**\n",
        "\t\t- It is a process of reducing dimensions of your feature sets, simply reducing the number of features.\n",
        "\t\t- They are mostly categorized as either \"Feature elimination\" or \"Feature extraction\"\n",
        "\t\t- Popular method is \"Principal Component Analysis (PCA)\" \n",
        "\n",
        "3. **Reinforcement Learning:**\n",
        "\t- It referes to the system learning through positive and negetive reinforcement on any action that it takes, wherein the reinforcement is provided by the external agent that helps the machine to learn.\n",
        "\t- this is an trial and error method.\n",
        "\n",
        "\t\n"
      ],
      "metadata": {
        "id": "7MSzaxH-TaYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised Learning Terminologies:\n",
        "> we are combining the input data and predict on never-before-seen data\n",
        ">\n",
        "\n",
        "1. Label \n",
        "  1. is the variable we're predicting\n",
        "  2. Typically represented by the variable y\n",
        "\n",
        "2. Features \n",
        "  1. are input variables describing our data\n",
        "  2. Typically represented by the variables {x1, x2, ..., xn}\n",
        "\n",
        "3. Example:\n",
        "  1. Labeled Example (x,y):\n",
        "    1. Used to train the model\n",
        "  2. Un-Labeled Example (x,?):\n",
        "    1. Used fpr making predictions on new data\n",
        "\n",
        "4. Model:\n",
        "  1. maps example to predict label y\n",
        "  1. defines the relationship between features and label\n",
        "  2. Defined by internal parameters which are learned.\n",
        "\n",
        "5. Phases of a model's life:\n",
        "  1. Training:\n",
        "    - means creating or learning the model. That is, you show the model labeled examples and enable the model to gradually learn the relationships between features and label.\n",
        "  2. Inference:\n",
        "    - means applying the trained model to unlabeled examples. That is, you use the trained model to make useful predictions (y)."
      ],
      "metadata": {
        "id": "3lHyB1KkN1At"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear regression:\n",
        "> is a method for finding the straight line or hyperplane that best fits a set of points. This module explores linear regression intuitively before laying the groundwork for a machine learning approach to linear regression.\n",
        ">\n",
        "\n",
        "1. y = w1x1 + b \n",
        "  1. b is for bias, (the y-intercept), sometimes referred to as w0. *line between 0 to from where straight line started on y axis*\n",
        "  2. w is for slop and weight factor\n",
        "\n",
        "2. **Loss**: \n",
        "  1. Its measures the distance between data elements and the straight line \n",
        "  2. Its a difference between true value and predicted value.\n",
        "  3. Loss is the penalty for a bad prediction. \n",
        "\n",
        "3. **L2 Loss**: \n",
        "  1. squared error\n",
        "  2. Square of the difference between prediction and label/true value\n",
        "\n",
        "4. **Mean square error (MSE)**:\n",
        "  - is the average squared loss per example over the whole dataset. To calculate MSE, sum up all the squared losses for individual examples and then divide by the number of examples\n",
        "  - Although MSE is commonly-used in machine learning, it is neither the only practical loss function nor the best loss function for all circumstances.\n",
        "\n",
        "4. **Goal of Training the Model**:\n",
        "  1. determining good values for all the weights and the bias from labeled examples.\n",
        "  2. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called **empirical risk minimization**.\n",
        "  3. The goal of training a model is **to find a set of weights and biases** that have low loss, on average, across all examples.\n",
        "\n",
        "5. **Reducing Loss**:\n"
      ],
      "metadata": {
        "id": "BuTaIMLUfuFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model Learning:\n",
        "\n",
        "1. Training Options:\n",
        "\t1. Max Allowed Iteration:\n",
        "\t2. Actual Iteration: "
      ],
      "metadata": {
        "id": "WxYW10YTT1PL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Metrix:\n",
        "Ref: https://medium.com/@skyl/evaluating-a-machine-learning-model-7cab1f597046\n",
        "1. Evaluation Metric:\n",
        "\t1. Accuracy: \n",
        "- Accuracy is, simply put, the total proportion of observations that have been correctly predicted. \n",
        "- (True Positive + True Negative) / (True Positive + True Negative + False Positive + False Negative)\n",
        "\t2. Precision: \n",
        "\t\t- This refers to the proportion (total number) of all observations that have been predicted to belong to the positive class and are actually positive.\n",
        "- True Positive / (True Positive + False Positive)\n",
        "3. Recall:\n",
        "\t- This is the proportion of observation predicted to belong to the positive class, that truly belongs to the positive class.\n",
        "\t- True Positive / (True Positive + False Negative)\n",
        "4. F1 Score:\n",
        "\t- is the weighted average of precision and recall between 0 and 1, where the ideal F-score value is 1. \n",
        "\t- (2 * Precision * Recall) / (Precision + Recall)\n",
        "5. Log Loss:\n",
        "\t- is a single score that represents the advantage of the classifier over a random prediction. The log loss measures the uncertainty of your model by comparing the probabilities of it’s outputs to the known values (ground truth).. You want to minimize log loss for the model as a whole.\n",
        "6. ROC AUC:\n",
        "\t- measures the area under the curve plotted with true positives on the y axis and false positives on the x axis. This metric is useful because it provides a single number that lets you compare models of different types.\n",
        "\n",
        "2. Confusion Metrix:\n",
        "\t- It shows how often the model classified each label correctly (in blue), and which labels were most often confused for that label (in grey).\n",
        "\t- TPR,FPR,FNR,TNR, Confusion Matrix: \n",
        "\t- Ref: https://www.youtube.com/watch?v=-dsoGHvk7II\n",
        "\n"
      ],
      "metadata": {
        "id": "NkvEYGx1No4U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlnmU7vBTT1h"
      },
      "outputs": [],
      "source": []
    }
  ]
}